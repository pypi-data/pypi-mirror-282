{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/01 13:31:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by local backend from path: obj_det/chkpts/mask_reppointsv2_swin_tiny_patch4_window7_3x.pth\n",
      "Done!!, save to obj_det/chkpts/mask_reppointsv2_swin_tiny_patch4_window7_3x_converted-dd7afe98.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:39: UserWarning: ``build_assigner`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_assigner`` would be deprecated soon, please use '\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:24: UserWarning: ``build_iou_calculator`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn(\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:46: UserWarning: ``build_sampler`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_sampler`` would be deprecated soon, please use '\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/apis/inference.py:90: UserWarning: dataset_meta or class names are not saved in the checkpoint's meta data, use COCO classes by default.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: obj_det/chkpts/mask_reppointsv2_swin_tiny_patch4_window7_3x_converted-dd7afe98.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:39: UserWarning: ``build_assigner`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_assigner`` would be deprecated soon, please use '\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:24: UserWarning: ``build_iou_calculator`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn(\n",
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/mmdet/models/task_modules/builder.py:46: UserWarning: ``build_sampler`` would be deprecated soon, please use ``mmdet.registry.TASK_UTILS.build()`` \n",
      "  warnings.warn('``build_sampler`` would be deprecated soon, please use '\n"
     ]
    }
   ],
   "source": [
    "# !mim download mmdet --config mask-rcnn_swin-t-p4-w7_fpn_1x_coco --dest .\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmengine.config import Config\n",
    "from cfm_task_models.legacy import *\n",
    "\n",
    "from cfm_task_models.split_utils import SplitSwinTransformer, SplitTwoStageDetector, TwoInputIdentity, SplitRepPointsV2MaskDetector\n",
    "\n",
    "config_file_old = 'obj_det/cfgs/swin_tiny_mask_rcnn_simplified_cfg.py'\n",
    "checkpoint_file_old = 'obj_det/chkpts/mask_rcnn_swin-t-p4-w7_fpn_1x_coco_20210902_120937-9d6b7cfa.pth'\n",
    "config_file = 'obj_det/cfgs/mask_reppointsv2_swin_t_modified.py'\n",
    "checkpoint_file = 'obj_det/chkpts/mask_reppointsv2_swin_tiny_patch4_window7_3x.pth'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model_init = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'\n",
    "# result = inference_detector(model, 'demo/demo.jpg')\n",
    "\n",
    "# from mmdet.registry import MODELS\n",
    "# MODELS.get('SwinTransformer')\n",
    "# MODELS.get('TwoStageDetector')\n",
    "model = SplitRepPointsV2MaskDetector.create_from_cfg_and_checkpoint(config_file, checkpoint_file).to(device)\n",
    "# model2 = SplitTwoStageDetector.create_from_cfg_and_checkpoint(config_file_old,checkpoint_file_old).to(device)\n",
    "# model.frontend_preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.runner import Runner\n",
    "from mmdet.structures import DetDataSample\n",
    "from typing import Union\n",
    "from torch import Tensor\n",
    "def to_device(data_sample:Union[dict,Tensor], device):\n",
    "    if isinstance(data_sample,dict):\n",
    "        for k,v in data_sample.items():\n",
    "            if isinstance(v, (Tensor, DetDataSample)):\n",
    "                data_sample[k] = v.to(device)\n",
    "                # print(f'{k} was sent to {device}')\n",
    "            elif isinstance(v, list):\n",
    "                new_v = [to_device(v2, device) for v2 in v]\n",
    "                data_sample[k] = new_v\n",
    "                # print(f'components of {k} were sent to {device}')\n",
    "            else:\n",
    "                print(f'{k} could not be sent to {device}')\n",
    "    elif isinstance(data_sample, (Tensor, DetDataSample)):\n",
    "        data_sample = data_sample.to(device)\n",
    "        # print(f'input was sent to {device}')\n",
    "    elif isinstance(data_sample, list):\n",
    "        data_sample = [to_device(v, device) for v in data_sample]\n",
    "        # print(f'components of input were sent to {device}')\n",
    "    else:\n",
    "        # print(type(data_sample))\n",
    "        print(f'input could not be sent to {device}')\n",
    "    return data_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local-scratch2/aharell/CFM-Task-Models/data/coco/annotations/coco2017_trn_split_anns.json\n",
      "loading annotations into memory...\n",
      "Done (t=12.94s)\n",
      "creating index...\n",
      "index created!\n",
      "/local-scratch2/aharell/CFM-Task-Models/data/coco/annotations/coco2017_val_split_anns.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.57s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.51s)\n",
      "creating index...\n",
      "index created!\n",
      "5915\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cfg_tr = model.cfg.copy()\n",
    "print(cfg_tr['train_dataloader']['dataset']['ann_file'])\n",
    "cfg_tr['train_dataloader']['batch_size'] = 1\n",
    "cfg_tr['train_dataloader']['num_workers'] = 1\n",
    "\n",
    "train_dataloader = Runner.build_dataloader(cfg_tr['train_dataloader']) \n",
    "\n",
    "print(cfg_tr['val_dataloader']['dataset']['ann_file'])\n",
    "val_dataloader = Runner.build_dataloader(cfg_tr['val_dataloader'])\n",
    "\n",
    "test_dataloader = Runner.build_dataloader(cfg_tr['test_dataloader'])\n",
    "print(len(val_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local-scratch2/aharell/CFM-Task-Models/data/coco/ annotations/instances_val2017.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.52s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/01 13:44:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=1.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=30.15s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=8.39s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.693\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.546\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.335\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.538\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.661\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.669\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.669\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.669\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.505\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.709\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.816\n",
      "04/01 13:44:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.504 0.693 0.546 0.335 0.538 0.661\n",
      "04/01 13:44:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating segm...\n",
      "Loading and preparing results...\n",
      "DONE (t=3.05s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=33.22s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=8.39s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.432\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.665\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.464\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.231\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.466\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.624\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.571\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.571\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.571\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.388\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.620\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.732\n",
      "04/01 13:45:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - segm_mAP_copypaste: 0.432 0.665 0.464 0.231 0.466 0.624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coco/bbox_mAP': 0.504,\n",
       " 'coco/bbox_mAP_50': 0.693,\n",
       " 'coco/bbox_mAP_75': 0.546,\n",
       " 'coco/bbox_mAP_s': 0.335,\n",
       " 'coco/bbox_mAP_m': 0.538,\n",
       " 'coco/bbox_mAP_l': 0.661,\n",
       " 'coco/segm_mAP': 0.432,\n",
       " 'coco/segm_mAP_50': 0.665,\n",
       " 'coco/segm_mAP_75': 0.464,\n",
       " 'coco/segm_mAP_s': 0.231,\n",
       " 'coco/segm_mAP_m': 0.466,\n",
       " 'coco/segm_mAP_l': 0.624}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dl = runner.train_dataloader\n",
    "# print(model.cut_point)\n",
    "# val_eval = Runner.build_evaluator(_,cfg_tr['val_evaluator'])\n",
    "# setattr(val_eval,'dataset_meta',val_dataloader.dataset.metainfo)\n",
    "from copy import deepcopy\n",
    "test_evaluator_cfg = deepcopy(cfg_tr['test_evaluator'])\n",
    "print(cfg_tr['train_dataloader']['dataset']['data_root'],cfg_tr['test_dataloader']['dataset']['ann_file'] )\n",
    "test_evaluator_cfg['ann_file'] = os.path.join(cfg_tr['test_dataloader']['dataset']['data_root'], test_evaluator_cfg['ann_file'])\n",
    "# # print(test_evaluator_cfg)\n",
    "test_eval = Runner.build_evaluator(_,cfg_tr['test_evaluator'])\n",
    "setattr(test_eval,'dataset_meta',test_dataloader.dataset.metainfo)\n",
    "# print(val_eval.dataset_meta)\n",
    "for i,data in enumerate(train_dataloader):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # data =  to_device(data,device)\n",
    "    # feat = model2.feature_frontend(data)\n",
    "    # print(feat['inputs']['outs'][0].shape)\n",
    "    # out2 = model2.test_step(feat)\n",
    "    # print(data['data_samples'][0])\n",
    "    if isinstance(model, SplitRepPointsV2MaskDetector):\n",
    "        if 'scale_factor' in data['data_samples'][0]:\n",
    "            for d in data['data_samples']:\n",
    "                meta = d.metainfo \n",
    "                # print(d)\n",
    "                meta['scale_factor'] = d.scale_factor * (2 if len(meta['scale_factor']) == 2 else 1)\n",
    "                d.set_metainfo(meta)\n",
    "\n",
    "    # im['inputs'] = torch.stack(im['inputs'])\n",
    "    # print(list(data_.keys()))\n",
    "    # data_ = train_pipeline(im)\n",
    "    # data_ = prp(data_, False)\n",
    "    # data_['inputs'] = model.backbone.split_forward_v2(data_['inputs'], output_layer=model.cut_point-1)\n",
    "    # data_['inputs'] = {\n",
    "    #                \"hw_shape\": data_['inputs'][0],\n",
    "    #\n",
    "    #                \"outs\": data_['inputs'][1]}\n",
    "    # data = prp(data, False)\n",
    "\n",
    "    \n",
    "    feat = model.feature_frontend(data)\n",
    "    # print(feat['inputs']['outs'][0].shape)\n",
    "    loss = model.backend_loss(feat)\n",
    "    if i > 100:\n",
    "        break\n",
    "    # out = model.test_step(feat)\n",
    "    # props = model.backend_raw(feat)\n",
    "    # print(model.cut_point)\n",
    "    # print(model.test_step(feat)[0])\n",
    "    # losses = model.loss(data_['inputs'], data_['data_samples'])\n",
    "    # loss, losses = model.parse_losses(losses)\n",
    "    # test_eval.process(out)\n",
    "# print(out[0].pred_instances.masks)\n",
    "# test_eval.evaluate(i+1)\n",
    "\n",
    "\n",
    "for i,data in enumerate(test_dataloader):\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # data =  to_device(data,device)\n",
    "        # feat = model2.feature_frontend(data)\n",
    "        # print(feat['inputs']['outs'][0].shape)\n",
    "        # out2 = model2.test_step(feat)\n",
    "        if isinstance(model, SplitRepPointsV2MaskDetector):\n",
    "            for d in data['data_samples']:\n",
    "                meta = d.metainfo \n",
    "                meta['scale_factor'] = d.scale_factor * (2 if len(meta['scale_factor']) == 2 else 1)\n",
    "                d.set_metainfo(meta)\n",
    "\n",
    "        # im['inputs'] = torch.stack(im['inputs'])\n",
    "        # print(list(data_.keys()))\n",
    "        # data_ = train_pipeline(im)\n",
    "        # data_ = prp(data_, False)\n",
    "        # data_['inputs'] = model.backbone.split_forward_v2(data_['inputs'], output_layer=model.cut_point-1)\n",
    "        # data_['inputs'] = {\n",
    "        #                \"hw_shape\": data_['inputs'][0],\n",
    "        #\n",
    "        #                \"outs\": data_['inputs'][1]}\n",
    "        # data = prp(data, False)\n",
    "\n",
    "        \n",
    "        feat = model.feature_frontend(data)\n",
    "        # print(feat['inputs']['outs'][0].shape)\n",
    "        out = model.test_step(feat)\n",
    "        # props = model.backend_raw(feat)\n",
    "        # print(model.cut_point)\n",
    "        # print(model.test_step(feat)[0])\n",
    "        # losses = model.loss(data_['inputs'], data_['data_samples'])\n",
    "        # loss, losses = model.parse_losses(losses)\n",
    "        test_eval.process(out)\n",
    "# print(out[0].pred_instances.masks)\n",
    "test_eval.evaluate(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39mint\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.62s)\n",
      "creating index...\n",
      "index created!\n",
      "03/29 14:51:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=6.81s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.08s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000\n",
      "03/29 14:51:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - bbox_mAP_copypaste: 0.001 0.001 0.001 0.000 0.000 0.000\n",
      "03/29 14:51:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Evaluating segm...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=6.45s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.08s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.000\n",
      "03/29 14:51:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - segm_mAP_copypaste: 0.000 0.001 0.001 0.000 0.000 0.000\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "val_eval = Runner.build_evaluator(_,cfg_tr['val_evaluator'])\n",
    "setattr(val_eval,'dataset_meta',val_dataloader.dataset.metainfo)\n",
    "\n",
    "# test_evaluator_cfg = deepcopy(cfg_tr['test_evaluator'])\n",
    "# print(train_dataset_cfg['data_root'],cfg_tr['test_dataloader']['dataset']['ann_file'] )\n",
    "# test_evaluator_cfg['ann_file'] = os.path.join(cfg_tr['test_dataloader']['dataset']['data_root'], test_evaluator_cfg['ann_file'])\n",
    "# print(test_evaluator_cfg)\n",
    "test_eval = Runner.build_evaluator(_,cfg_tr['test_evaluator'])\n",
    "setattr(test_eval,'dataset_meta',test_dataloader.dataset.metainfo)\n",
    "# print(val_eval.dataset_meta)\n",
    "# print(out)\n",
    "test_eval.process(out)\n",
    "test_eval.evaluate(1)\n",
    "print(test_eval.metrics[0].results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Instances()\n",
    "\n",
    "fk = torch.Tensor([[[3,4]],[[4,5]],[[5,6]]])\n",
    "device = 'cuda'\n",
    "a.fpn_levels = torch.cat([torch.tensor([i]*r[:,0].numel()).to(device) for i,r in enumerate(fk)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0.,   0.,   1., ...,  34.,   0.,   0.],\n",
       "        [  0.,   0.,   0., ...,  91.,   2.,   0.],\n",
       "        [  0.,   0.,   0., ...,  47.,   0.,   0.],\n",
       "        ...,\n",
       "        [  5.,  92.,  22., ...,  41.,   5.,   0.],\n",
       "        [  0.,   0.,   0., ...,  57.,   4.,   0.],\n",
       "        [  0.,   0.,   0., ..., 122.,   1.,   0.]]),\n",
       " array([0.        , 0.1       , 0.2       , 0.30000001, 0.40000001,\n",
       "        0.5       , 0.60000002, 0.69999999, 0.80000001, 0.89999998,\n",
       "        1.        ]),\n",
       " <a list of 2850 BarContainer objects>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlbklEQVR4nO3df3BU9b3/8Vd+kARpNiE42SXX8EOqAhq1khIX/FXJEAW8MPKtMKYUW0paTewFZlBS+ZEGFU2RpnCjXLwKOBdK653KVeSm0iBQJQSMpKWQohZ6ifVuaAeThSibX+f7Ry+nLATJxv2Rz+b5mNkZc85ns+8cA3myOWc3xrIsSwAAAAaJjfQAAAAAgSJgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABgnPtIDhEpnZ6c++eQTJScnKyYmJtLjAACAbrAsS6dPn1ZGRoZiYy/9PEvUBswnn3yizMzMSI8BAAB6oKGhQVddddUl90dtwCQnJ0v6+wFwOBwRngYAAHSH1+tVZmam/XP8UqI2YM792sjhcBAwAAAY5nKnf3ASLwAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwACAYVxv10V6BCDiCBgA6AVKSkoiPQJgFAIGAHqJ52ZM6ZOPDfQEAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwBRqn7kqEiPAIQMAQMAAIxDwAAAAOMQMABgIF74Dn0dAQMA0awkJdITACFBwAAAAOMQMAAAwDgBB8yePXt03333KSMjQzExMdq6davffsuytHTpUg0ePFj9+/dXbm6uPvzwQ781p06dUn5+vhwOh1JTUzVnzhydOXPGb83vf/973X777UpKSlJmZqbKysoC/+oAAEBUCjhgWlpadNNNN6mioqLL/WVlZVq9erXWrl2rmpoaDRgwQHl5eTp79qy9Jj8/X4cPH9aOHTu0bds27dmzRwUFBfZ+r9eriRMnaujQoaqtrdVPfvITlZSUaN26dT34EgEAQLSJD/QO9957r+69994u91mWpfLyci1evFhTp06VJL3yyityOp3aunWrZs6cqfr6elVWVurAgQPKzs6WJK1Zs0aTJk3SypUrlZGRoU2bNqm1tVUvv/yyEhISdP3116uurk6rVq3yCx0AQOCqdo7QhLv/dNH2+pGjNOqP9RGYCAhcUM+BOX78uDwej3Jzc+1tKSkpysnJUXV1tSSpurpaqampdrxIUm5urmJjY1VTU2OvueOOO5SQkGCvycvL09GjR/Xpp592+dg+n09er9fvBgC4AFclIUoENWA8Ho8kyel0+m13Op32Po/Ho/T0dL/98fHxSktL81vT1ec4/zEutGLFCqWkpNi3zMzML/8FAcD/ydqYFekRAJwnaq5CKi4uVnNzs31raGiI9EgAEFYVP9j5hft5byREk6AGjMvlkiQ1Njb6bW9sbLT3uVwunTx50m9/e3u7Tp065bemq89x/mNcKDExUQ6Hw+8GAACiU1ADZvjw4XK5XKqqqrK3eb1e1dTUyO12S5LcbreamppUW1trr9m5c6c6OzuVk5Njr9mzZ4/a2trsNTt27NB1112ngQMHBnNkALisyz2zASD8Ag6YM2fOqK6uTnV1dZL+fuJuXV2dTpw4oZiYGM2bN09PPvmkXn/9dR06dEjf/va3lZGRoWnTpkmSRo0apXvuuUdz587V/v379e6776qoqEgzZ85URkaGJOnBBx9UQkKC5syZo8OHD+sXv/iFfvazn2nBggVB+8IBAIC5Ar6M+r333tM3vvEN++NzUTF79mxt2LBBjz32mFpaWlRQUKCmpibddtttqqysVFJSkn2fTZs2qaioSBMmTFBsbKymT5+u1atX2/tTUlL01ltvqbCwUGPGjNGVV16ppUuXcgk1gD4na2OWDs0+FPD9Pl702xBMA/QeAQfMXXfdJcuyLrk/JiZGpaWlKi0tveSatLQ0bd68+Qsf58Ybb9Rvf8sfQAAAcLGouQoJAAD0HQQMgKhStXNEpEcAEAYEDACod75Q3XMzply07aI5eWVd9FEEDAAAMA4BAyCq8SslIDoRMAAQYcMWvdmtdV/0gnpcNo2+hoABgP/T3ZD4Ir01JHrrXEBPETAA+qxgBEs04NdsMBEBAwBRjkBBNCJgAEQdfl0CRD8CBgAC1KvfnZrXhUEfQcAAQB/GeUAwFQEDABFUP3JUpEcAjBTwu1EDAEJr2KI39VDSASX38P6ut+v0XlAnAnofnoEBgD6oq/dZkviVEsxBwACA4Vxv10V6BCDsCBgAAGAcAgYADNWrL+cGQoyAAdBncH4HED0IGADoRbI2ZkV6BMAIBAyAPsXU9wUqKSmJ9AhAr0LAAIgKnA8SOF5EDyYjYACgC/wqB+jdCBgAAGAcAgYAABiHgAGAXo5X2gUuRsAAAADjEDAAAMA4BAwAfAkfL/ptpEcA+iQCBgAAGIeAAYAviRfRA8KPgAEAAMYhYAAgALwnEdA7EDAAAMA4BAyAqMQbFQLRjYABAADGIWAAoJu6+5L+PPsDhB4BAwAAjEPAAAAA4xAwAADAOAQMAFzguRlTIj0CgMsgYAD0aSF7YbqSlNB8XgCSCBgA0YyIAKIWAQMAAIxDwAAAAOMQMACiHiflAtGHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAYAe8nt3al5zBggrAgZAn8dVSoB5CBgAAGAcAgYAABgn6AHT0dGhJUuWaPjw4erfv79GjBih5cuXy7Ise41lWVq6dKkGDx6s/v37Kzc3Vx9++KHf5zl16pTy8/PlcDiUmpqqOXPm6MyZM8EeF0AfUT9yVKRHABBEQQ+YZ599Vi+88IL+9V//VfX19Xr22WdVVlamNWvW2GvKysq0evVqrV27VjU1NRowYIDy8vJ09uxZe01+fr4OHz6sHTt2aNu2bdqzZ48KCgqCPS4A9Awn7QIRFR/sT7h3715NnTpVkydPliQNGzZMP//5z7V//35Jf3/2pby8XIsXL9bUqVMlSa+88oqcTqe2bt2qmTNnqr6+XpWVlTpw4ICys7MlSWvWrNGkSZO0cuVKZWRkBHtsAABgkKA/AzNu3DhVVVXpgw8+kCT97ne/0zvvvKN7771XknT8+HF5PB7l5uba90lJSVFOTo6qq6slSdXV1UpNTbXjRZJyc3MVGxurmpqaYI8MIFp081kRrjoCzBf0Z2AWLVokr9erkSNHKi4uTh0dHXrqqaeUn58vSfJ4PJIkp9Ppdz+n02nv83g8Sk9P9x80Pl5paWn2mgv5fD75fD77Y6/XG7SvCQAA9C5Bfwbml7/8pTZt2qTNmzfr/fff18aNG7Vy5Upt3Lgx2A/lZ8WKFUpJSbFvmZmZIX08AH1DSUlJpEcA0IWgB8zChQu1aNEizZw5U1lZWZo1a5bmz5+vFStWSJJcLpckqbGx0e9+jY2N9j6Xy6WTJ0/67W9vb9epU6fsNRcqLi5Wc3OzfWtoaAj2lwagDzg/WPxeabcbAl0PoOeCHjCfffaZYmP9P21cXJw6OzslScOHD5fL5VJVVZW93+v1qqamRm63W5LkdrvV1NSk2tpae83OnTvV2dmpnJycLh83MTFRDofD7wYAAKJT0M+Bue+++/TUU09pyJAhuv7663Xw4EGtWrVK3/3udyVJMTExmjdvnp588kldc801Gj58uJYsWaKMjAxNmzZNkjRq1Cjdc889mjt3rtauXau2tjYVFRVp5syZXIEEICjqR46Spq2M9BgAeijoAbNmzRotWbJEjzzyiE6ePKmMjAx9//vf19KlS+01jz32mFpaWlRQUKCmpibddtttqqysVFJSkr1m06ZNKioq0oQJExQbG6vp06dr9erVwR4XAAAYKOgBk5ycrPLycpWXl19yTUxMjEpLS1VaWnrJNWlpadq8eXOwxwMAAFGA90ICAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYJyQBMxf/vIXfetb39KgQYPUv39/ZWVl6b333rP3W5alpUuXavDgwerfv79yc3P14Ycf+n2OU6dOKT8/Xw6HQ6mpqZozZ47OnDkTinEBAIBhgh4wn376qcaPH69+/frpv//7v3XkyBE999xzGjhwoL2mrKxMq1ev1tq1a1VTU6MBAwYoLy9PZ8+etdfk5+fr8OHD2rFjh7Zt26Y9e/aooKAg2OMCAAADxQf7Ez777LPKzMzU+vXr7W3Dhw+3/9uyLJWXl2vx4sWaOnWqJOmVV16R0+nU1q1bNXPmTNXX16uyslIHDhxQdna2JGnNmjWaNGmSVq5cqYyMjGCPDQAADBL0Z2Bef/11ZWdn65vf/KbS09P1ta99TS+++KK9//jx4/J4PMrNzbW3paSkKCcnR9XV1ZKk6upqpaam2vEiSbm5uYqNjVVNTU2Xj+vz+eT1ev1uAAAgOgU9YI4dO6YXXnhB11xzjX7961/r4Ycf1g9/+ENt3LhRkuTxeCRJTqfT735Op9Pe5/F4lJ6e7rc/Pj5eaWlp9poLrVixQikpKfYtMzMz2F8aAADoJYIeMJ2dnbrlllv09NNP62tf+5oKCgo0d+5crV27NtgP5ae4uFjNzc32raGhIaSPBwAAIifoATN48GCNHj3ab9uoUaN04sQJSZLL5ZIkNTY2+q1pbGy097lcLp08edJvf3t7u06dOmWvuVBiYqIcDoffDQAARKegB8z48eN19OhRv20ffPCBhg4dKunvJ/S6XC5VVVXZ+71er2pqauR2uyVJbrdbTU1Nqq2ttdfs3LlTnZ2dysnJCfbIAADAMEG/Cmn+/PkaN26cnn76aT3wwAPav3+/1q1bp3Xr1kmSYmJiNG/ePD355JO65pprNHz4cC1ZskQZGRmaNm2apL8/Y3PPPffYv3pqa2tTUVGRZs6cyRVIAAAg+AHz9a9/Xa+99pqKi4tVWlqq4cOHq7y8XPn5+faaxx57TC0tLSooKFBTU5Nuu+02VVZWKikpyV6zadMmFRUVacKECYqNjdX06dO1evXqYI8LAAAMFPSAkaQpU6ZoypQpl9wfExOj0tJSlZaWXnJNWlqaNm/eHIrxAACA4XgvJAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYh4ABAADGIWAAAIBxCBgAAGAcAgYAABiHgAEAAMYhYAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYAABgHAIGAAAYJ+QB88wzzygmJkbz5s2zt509e1aFhYUaNGiQvvKVr2j69OlqbGz0u9+JEyc0efJkXXHFFUpPT9fChQvV3t4e6nEBAIABQhowBw4c0L/927/pxhtv9Ns+f/58vfHGG3r11Ve1e/duffLJJ7r//vvt/R0dHZo8ebJaW1u1d+9ebdy4URs2bNDSpUtDOS4AADBEyALmzJkzys/P14svvqiBAwfa25ubm/XSSy9p1apVuvvuuzVmzBitX79ee/fu1b59+yRJb731lo4cOaL/+I//0M0336x7771Xy5cvV0VFhVpbW0M1MgAAMETIAqawsFCTJ09Wbm6u3/ba2lq1tbX5bR85cqSGDBmi6upqSVJ1dbWysrLkdDrtNXl5efJ6vTp8+HCXj+fz+eT1ev1uAAAgOsWH4pNu2bJF77//vg4cOHDRPo/Ho4SEBKWmpvptdzqd8ng89prz4+Xc/nP7urJixQr9+Mc/DsL0AACgtwv6MzANDQ36l3/5F23atElJSUnB/vSXVFxcrObmZvvW0NAQtscGAADhFfSAqa2t1cmTJ3XLLbcoPj5e8fHx2r17t1avXq34+Hg5nU61traqqanJ736NjY1yuVySJJfLddFVSec+PrfmQomJiXI4HH43AAAQnYIeMBMmTNChQ4dUV1dn37Kzs5Wfn2//d79+/VRVVWXf5+jRozpx4oTcbrckye1269ChQzp58qS9ZseOHXI4HBo9enSwRwYAAIYJ+jkwycnJuuGGG/y2DRgwQIMGDbK3z5kzRwsWLFBaWpocDoceffRRud1u3XrrrZKkiRMnavTo0Zo1a5bKysrk8Xi0ePFiFRYWKjExMdgjAwAAw4TkJN7L+elPf6rY2FhNnz5dPp9PeXl5ev755+39cXFx2rZtmx5++GG53W4NGDBAs2fPVmlpaSTGBQAAvUxYAmbXrl1+HyclJamiokIVFRWXvM/QoUO1ffv2EE8GAABMxHshAQAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4QQ+YFStW6Otf/7qSk5OVnp6uadOm6ejRo35rzp49q8LCQg0aNEhf+cpXNH36dDU2NvqtOXHihCZPnqwrrrhC6enpWrhwodrb24M9LgAAMFDQA2b37t0qLCzUvn37tGPHDrW1tWnixIlqaWmx18yfP19vvPGGXn31Ve3evVuffPKJ7r//fnt/R0eHJk+erNbWVu3du1cbN27Uhg0btHTp0mCPCwAADBQf7E9YWVnp9/GGDRuUnp6u2tpa3XHHHWpubtZLL72kzZs36+6775YkrV+/XqNGjdK+fft066236q233tKRI0f0m9/8Rk6nUzfffLOWL1+uxx9/XCUlJUpISAj22AAAwCAhPwemublZkpSWliZJqq2tVVtbm3Jzc+01I0eO1JAhQ1RdXS1Jqq6uVlZWlpxOp70mLy9PXq9Xhw8f7vJxfD6fvF6v3w0AAESnkAZMZ2en5s2bp/Hjx+uGG26QJHk8HiUkJCg1NdVvrdPplMfjsdecHy/n9p/b15UVK1YoJSXFvmVmZgb5qwEAAL1FSAOmsLBQf/jDH7Rly5ZQPowkqbi4WM3NzfatoaEh5I8JAAAiI+jnwJxTVFSkbdu2ac+ePbrqqqvs7S6XS62trWpqavJ7FqaxsVEul8tes3//fr/Pd+4qpXNrLpSYmKjExMQgfxUAAKA3CvozMJZlqaioSK+99pp27typ4cOH++0fM2aM+vXrp6qqKnvb0aNHdeLECbndbkmS2+3WoUOHdPLkSXvNjh075HA4NHr06GCPDAAADBP0Z2AKCwu1efNm/dd//ZeSk5Ptc1ZSUlLUv39/paSkaM6cOVqwYIHS0tLkcDj06KOPyu1269Zbb5UkTZw4UaNHj9asWbNUVlYmj8ejxYsXq7CwkGdZAABA8APmhRdekCTdddddftvXr1+vhx56SJL005/+VLGxsZo+fbp8Pp/y8vL0/PPP22vj4uK0bds2Pfzww3K73RowYIBmz56t0tLSYI8LAAAMFPSAsSzrsmuSkpJUUVGhioqKS64ZOnSotm/fHszRAABAlOC9kAAAgHEIGAAAYBwCBgAAGIeAAQAAxiFgAACAcQgYABGVtTEr0iMAMBABAwAAjEPAAAAA4xAwAADAOAQMAAAwDgEDIOKGLXoz0iMAMAwBAwAAjEPAAAAA4xAwAADAOAQMAKDXem7GlEiPgF6KgAHQbR8v+m2kRwC65Hq7LtIjIMwIGAC9Dj+MAFwOAQMgIBU/2NmtdVU7R4R4EkS7kpKSSI+AXoyAAQB8eSUpkZ4AfQwBA+CSgvEv4PqRo+z/fm7GFL+PAaCnCBgAvRK/PohyPGODL4mAAdCl7p5IyzMqOIfvGYQTAQMAAIxDwADokVBc6syLlgHoLgIGQOBCcP4C57xAImLRfQQMgJDhhxFsnLSLICNgAHx5/HDqk1xv14XkmbPunuTLs3Z9GwEDAACMQ8AgIIFc/sivDwAAoULAAAgL3hup7wrF677whp8gYCCJvwxwad1980ZJ+njRb7u1btiiN3s6DgBIImAQIpxch2DJ2pgV6REA9EIEDALHFSdRr7vRwDMuACKFgAEAAMYhYAD4CeScl1DgV0YAuoOAARAUobjKKNIxBaD3ImAAdEt3r1TjVVQBhAMBAwAAjEPAALg8rjxDN51/ZVowX1+KK9lwIQIGQPB1M3h4u4noxLlLCAcCBgAAGIeAAQAYg2ftcA4BAwCIiHOX3ofizR4R/QgYAEDALvuCg0E88ZsXN0RXCBgAgBF47SCcj4ABAITNhW8Aev45Ld19NecLn5HhGZq+iYABAADGIWAAAIBxCBgAQMic/yujQN7ws7uvvMuL5vVdBAwA4EshIhAJBAyMFIrXjXC9Xdetqxx6y2tWBDpHb5kb6ArvdYRAETCIaiUlJb3mlTsDmYPLRQHgixEwMFeUvkNyMN/B95zeEnEAECwEDGCyKI04ALicXh0wFRUVGjZsmJKSkpSTk6P9+/dHeiQgIs6/kiMUz9AAgGl6bcD84he/0IIFC7Rs2TK9//77uummm5SXl6eTJ09GerTL6vb5CwH865kTMKNTKF5BNJBLVQH8A3/PmqXXBsyqVas0d+5cfec739Ho0aO1du1aXXHFFXr55ZcjPVq3dPecA/41je5+D1T8YGe3o/e5GVP4yxgIkVD8IxWBi4/0AF1pbW1VbW2tiouL7W2xsbHKzc1VdXV1l/fx+Xzy+Xz2x83NzZIkr9cb2mEvMUu/trbLP7bPUmfLmW7NeKajQ95ih1T8cZCm9BfQHD5Luszabh+DHgrFHJ0tZ+Tz+S679kxHR7c+35qHvqmJh45r+pSn9Icf5+lsF3N0fN6hzpYzKi4u1qDGcXp57OPqvGqdfD6fVn73DRWU3ylJOu1r0eetbfL+3/dM/fzKSx6DlpZOnW1r05mODg2Z/6r92N2d+/PWFnsun8+nz1tb7Me9cI7TvhZ7jpaWzos+Pve4nb7PLvrYF/P3/zcdn3fx8XmP3fF59+cO9RyBHpPLfX9KCsscgR6T7hzv3vY90t2/w7qya/dNOrT+Wk28zJ+RWzffqn0P7pPP59NT9+fp0Q2vXrTmq3t+r4/uuFHr5u1WgdPSV7e9o4/uuPGidUfHZOu6/+cJ2d/pJjv3/8CyrC9eaPVCf/nLXyxJ1t69e/22L1y40Bo7dmyX91m2bJkliRs3bty4ceMWBbeGhoYvbIVe+QxMTxQXF2vBggX2x52dnTp16pQGDRqkmJiYHn9er9erzMxMNTQ0yOFwBGNUfAGOd3hxvMOL4x1eHO/wCtbxtixLp0+fVkZGxheu65UBc+WVVyouLk6NjY1+2xsbG+Vyubq8T2JiohITE/22paamBm0mh8PBH4Aw4niHF8c7vDje4cXxDq9gHO+UlJTLrumVJ/EmJCRozJgxqqqqsrd1dnaqqqpKbrc7gpMBAIDeoFc+AyNJCxYs0OzZs5Wdna2xY8eqvLxcLS0t+s53vhPp0QAAQIT12oCZMWOG/vrXv2rp0qXyeDy6+eabVVlZKafTGdY5EhMTtWzZsot+PYXQ4HiHF8c7vDje4cXxDq9wH+8Yy7rcdUoAAAC9S688BwYAAOCLEDAAAMA4BAwAADAOAQMAAIxDwEiqqKjQsGHDlJSUpJycHO3fv/8L17/66qsaOXKkkpKSlJWVpe3bt4dp0ugQyPF+8cUXdfvtt2vgwIEaOHCgcnNzL/v/B/4C/f4+Z8uWLYqJidG0adNCO2CUCfR4NzU1qbCwUIMHD1ZiYqKuvfZa/k4JQKDHu7y8XNddd5369++vzMxMzZ8/X2fPng3TtObas2eP7rvvPmVkZCgmJkZbt2697H127dqlW265RYmJifrqV7+qDRs2BHeo4Lx7kbm2bNliJSQkWC+//LJ1+PBha+7cuVZqaqrV2NjY5fp3333XiouLs8rKyqwjR45Yixcvtvr162cdOnQozJObKdDj/eCDD1oVFRXWwYMHrfr6euuhhx6yUlJSrI8//jjMk5sp0ON9zvHjx61/+qd/sm6//XZr6tSp4Rk2CgR6vH0+n5WdnW1NmjTJeuedd6zjx49bu3btsurq6sI8uZkCPd6bNm2yEhMTrU2bNlnHjx+3fv3rX1uDBw+25s+fH+bJzbN9+3briSeesH71q19ZkqzXXnvtC9cfO3bMuuKKK6wFCxZYR44csdasWWPFxcVZlZWVQZupzwfM2LFjrcLCQvvjjo4OKyMjw1qxYkWX6x944AFr8uTJfttycnKs73//+yGdM1oEerwv1N7ebiUnJ1sbN24M1YhRpSfHu7293Ro3bpz17//+79bs2bMJmAAEerxfeOEF6+qrr7ZaW1vDNWJUCfR4FxYWWnfffbfftgULFljjx48P6ZzRpjsB89hjj1nXX3+937YZM2ZYeXl5QZujT/8KqbW1VbW1tcrNzbW3xcbGKjc3V9XV1V3ep7q62m+9JOXl5V1yPf6hJ8f7Qp999pna2tqUlpYWqjGjRk+Pd2lpqdLT0zVnzpxwjBk1enK8X3/9dbndbhUWFsrpdOqGG27Q008/rY6OjnCNbayeHO9x48aptrbW/jXTsWPHtH37dk2aNCksM/cl4fhZ2WtfiTcc/va3v6mjo+OiV/d1Op364x//2OV9PB5Pl+s9Hk/I5owWPTneF3r88ceVkZFx0R8MXKwnx/udd97RSy+9pLq6ujBMGF16cryPHTumnTt3Kj8/X9u3b9dHH32kRx55RG1tbVq2bFk4xjZWT473gw8+qL/97W+67bbbZFmW2tvb9YMf/EA/+tGPwjFyn3Kpn5Ver1eff/65+vfv/6Ufo08/AwOzPPPMM9qyZYtee+01JSUlRXqcqHP69GnNmjVLL774oq688spIj9MndHZ2Kj09XevWrdOYMWM0Y8YMPfHEE1q7dm2kR4tKu3bt0tNPP63nn39e77//vn71q1/pzTff1PLlyyM9GnqgTz8Dc+WVVyouLk6NjY1+2xsbG+Vyubq8j8vlCmg9/qEnx/uclStX6plnntFvfvMb3XjjjaEcM2oEerz/9Kc/6c9//rPuu+8+e1tnZ6ckKT4+XkePHtWIESNCO7TBevL9PXjwYPXr109xcXH2tlGjRsnj8ai1tVUJCQkhndlkPTneS5Ys0axZs/S9731PkpSVlaWWlhYVFBToiSeeUGws/6YPlkv9rHQ4HEF59kXq48/AJCQkaMyYMaqqqrK3dXZ2qqqqSm63u8v7uN1uv/WStGPHjkuuxz/05HhLUllZmZYvX67KykplZ2eHY9SoEOjxHjlypA4dOqS6ujr79s///M/6xje+obq6OmVmZoZzfOP05Pt7/Pjx+uijj+xQlKQPPvhAgwcPJl4uoyfH+7PPPrsoUs7Fo8XbAgZVWH5WBu10YENt2bLFSkxMtDZs2GAdOXLEKigosFJTUy2Px2NZlmXNmjXLWrRokb3+3XffteLj462VK1da9fX11rJly7iMOgCBHu9nnnnGSkhIsP7zP//T+t///V/7dvr06Uh9CUYJ9HhfiKuQAhPo8T5x4oSVnJxsFRUVWUePHrW2bdtmpaenW08++WSkvgSjBHq8ly1bZiUnJ1s///nPrWPHjllvvfWWNWLECOuBBx6I1JdgjNOnT1sHDx60Dh48aEmyVq1aZR08eND6n//5H8uyLGvRokXWrFmz7PXnLqNeuHChVV9fb1VUVHAZdSisWbPGGjJkiJWQkGCNHTvW2rdvn73vzjvvtGbPnu23/pe//KV17bXXWgkJCdb1119vvfnmm2Ge2GyBHO+hQ4daki66LVu2LPyDGyrQ7+/zETCBC/R4792718rJybESExOtq6++2nrqqaes9vb2ME9trkCOd1tbm1VSUmKNGDHCSkpKsjIzM61HHnnE+vTTT8M/uGHefvvtLv8uPnd8Z8+ebd15550X3efmm2+2EhISrKuvvtpav359UGeKsSyeNwMAAGbp0+fAAAAAMxEwAADAOAQMAAAwDgEDAACMQ8AAAADjEDAAAMA4BAwAADAOAQMAAIxDwAAAAOMQMAAAwDgEDAAAMA4BAwAAjPP/Adb5K+mcYjmEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "mx = feat['inputs']['outs'][0][0,:].max().item()\n",
    "mn = feat['inputs']['outs'][0][0,:].min().item()\n",
    "f_np = feat['inputs']['outs'][0][0,:].reshape(2048,-1).cpu().detach()\n",
    "f_np = f_np.detach().numpy()\n",
    "# print(f_np)\n",
    "plt.hist((f_np-mn)/(mx-mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SplitSwinTransformer' object has no attribute 'norm0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# feat2 = feat['inputs']['outs'][0].view(-1, *feat['inputs']['hw_shape'],\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#                                 model.backbone.num_features[0]).permute(0, 3, 1,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#                                                                 2).contiguous()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# print(feat2.shape)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m norm0 \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(model\u001b[39m.\u001b[39;49mbackbone, \u001b[39m'\u001b[39;49m\u001b[39mnorm0\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m feats_to_comp  \u001b[39m=\u001b[39m norm0( feat[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mouts\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39mfeat[\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mhw_shape\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m                                 model\u001b[39m.\u001b[39mbackbone\u001b[39m.\u001b[39mnum_features[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                                                 \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(feats_to_comp\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/local-scratch2/aharell/CFM-Task-Models/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SplitSwinTransformer' object has no attribute 'norm0'"
     ]
    }
   ],
   "source": [
    "# feat2 = feat['inputs']['outs'][0].view(-1, *feat['inputs']['hw_shape'],\n",
    "#                                 model.backbone.num_features[0]).permute(0, 3, 1,\n",
    "#                                                                 2).contiguous()\n",
    "# print(feat2.shape)\n",
    "norm0 = getattr(model.backbone, 'norm0')\n",
    "feats_to_comp  = norm0( feat['inputs']['outs'][0]).view(-1, *feat['inputs']['hw_shape'],\n",
    "                                model.backbone.num_features[0]).permute(0, 3, 1,\n",
    "                                                                2).contiguous()\n",
    "print(feats_to_comp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in vdl:\n",
    "    # model.cut_point = 0\n",
    "    # model.data_preprocessor = model.frontend_preprocessor\n",
    "    # print(model.val_step(data))\n",
    "    # model.cut_point = 1\n",
    "    # model.data_preprocessor = TwoInputIdentity()\n",
    "    data_ = model.frontend_preprocessor(data, False)\n",
    "    print(data_['data_samples'])\n",
    "    break\n",
    "    data_['inputs'] = model.backbone.split_forward_v2(data_['inputs'], output_layer=model.cut_point-1)\n",
    "    data_['inputs'] = {#\"x\": data_['inputs'][0],\n",
    "                    \"hw_shape\": data_['inputs'][0],\n",
    "                    \"outs\": data_['inputs'][1]}\n",
    "    print(len(data_['inputs']['outs']))\n",
    "    print(data_)\n",
    "    # data_['inputs'] = [data_['inputs']]\n",
    "    # data_['data_samples'] = [data_['data_samples']]\n",
    "    # # forward the model\n",
    "    with torch.no_grad():\n",
    "        results = model.backend_inference(data_)[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdl = runner.val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone.stages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1442085560638556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.999999999998509"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import log2 as log\n",
    "def H(d):\n",
    "    return -d*log(d) + (d-1)*log(1-d)\n",
    "\n",
    "m = 8\n",
    "D = np.linspace(0.1442,0.14421,10000000)\n",
    "# R = 1-D*m/(m-1)\n",
    "# rate = R*log(m)\n",
    "the_rate = log(m) -H(D) - D * log(m-1) - 2\n",
    "print(D[np.where(the_rate < 0)[0][0]])\n",
    "the_rate[np.where(the_rate < 0)[0][0]] + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mmseg.apis import inference_model, init_model, show_result_pyplot\n",
    "import mmcv\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from mmengine.config import Config\n",
    "from mmengine.dataset import Compose\n",
    "\n",
    "from utils import SplitEncoderDecoder, TwoInputIdentity\n",
    "\n",
    "config_file = \"../../segmentation/maskformer_swin-t_upernet_8xb2-160k_ade20k-512x512_modified.py\"\n",
    "checkpoint_file = \"../../segmentation/maskformer_swin-t_upernet_8xb2-160k_ade20k-512x512_20221114_232813-f14e7ce0.pth\"\n",
    "\n",
    "\n",
    "# Build the model from a config file and a checkpoint file\n",
    "model_init = init_model(config_file, checkpoint_file, device='cpu') # or device='cuda:0'\n",
    "\n",
    "# test a single image and show the results\n",
    "img = '../../demo/demo.jpg'  # or img = mmcv.imread(img), which will only load it once\n",
    "result = inference_model(model_init, img)\n",
    "# # visualize the results in a new window\n",
    "# # show_result_pyplot(model, img, result, show=True)\n",
    "# # or save the visualization results to image files\n",
    "# # you can change the opacity of the painted segmentation map in (0, 1].\n",
    "# show_result_pyplot(model, img, result, show=True, out_file='result.jpg', opacity=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from utils import SplitEncoderDecoder #, get_mean_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the config file\n",
    "cfg = Config.fromfile('../../segmentation/swin-tiny-patch4-window7-in1k-pre_upernet_8xb2-160k_ade20k-512x512.py')\n",
    "\n",
    "# set the data_root of the config file\n",
    "data_root = '../../data/ade/ADEChallengeData2016'\n",
    "cfg.data_root = data_root\n",
    "cfg.test_dataloader.dataset.data_root = data_root\n",
    "cfg.train_dataloader.dataset.data_root = data_root\n",
    "cfg.val_dataloader.dataset.data_root = data_root\n",
    "\n",
    "\n",
    "train_pipeline = [{'type': 'LoadImageFromFile'},\n",
    "                  {'reduce_zero_label': True, 'type': 'LoadAnnotations'},\n",
    "                  {'keep_ratio': True,\n",
    "                   'ratio_range': (0.5, 2.0),\n",
    "                   'scale': (2048, 1024), # (2048, 512)\n",
    "                   'type': 'RandomResize'},\n",
    "                   {'cat_max_ratio': 0.75, 'crop_size': (512, 512), 'type': 'RandomCrop'},\n",
    "                   {'prob': 0.5, 'type': 'RandomFlip'},\n",
    "                   {'type': 'PhotoMetricDistortion'},\n",
    "                   {'type': 'PackSegInputs'}]\n",
    "\n",
    "cfg.train_dataloader.dataset.pipeline = train_pipeline\n",
    "\n",
    "cfg.dump('../../segmentation/swin-tiny-patch4-window7-in1k-pre_upernet_8xb2-160k_ade20k-512x512_modified.py')\n",
    "\n",
    "config_file = \"../../segmentation/swin-tiny-patch4-window7-in1k-pre_upernet_8xb2-160k_ade20k-512x512_modified.py\"\n",
    "checkpoint_file = '../../segmentation/upernet_swin_tiny_patch4_window7_512x512_160k_ade20k_pretrain_224x224_1K_20210531_112542-e380ad3e.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_file = \"../../segmentation/swin-tiny-patch4-window7-in1k-pre_upernet_8xb2-160k_ade20k-512x512_modified.py\"\n",
    "# checkpoint_file = '../../segmentation/upernet_swin_tiny_patch4_window7_512x512_160k_ade20k_pretrain_224x224_1K_20210531_112542-e380ad3e.pth'\n",
    "\n",
    "# model = SplitEncoderDecoder.create_from_cfg_and_checkpoint(config_file, checkpoint_file)\n",
    "\n",
    "# cfg['work_dir'] = './logs'\n",
    "# runner = Runner.from_cfg(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SplitEncoderDecoder.create_from_cfg_and_checkpoint(config_file, checkpoint_file)\n",
    "# dataloader = Runner.build_dataloader(cfg.val_dataloader)\n",
    "\n",
    "# cut point = 0, 1 test\n",
    "\n",
    "\n",
    "model = SplitEncoderDecoder.create_from_cfg_and_checkpoint(config_file, checkpoint_file)\n",
    "# model.set_cut_point(0) # so that input_layer=self.cut_point=0\n",
    "# model.swap_preprocessor()\n",
    "model.zero_grad()\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "\n",
    "dataloader = Runner.build_dataloader(cfg.val_dataloader)\n",
    "val_eval = Runner.build_evaluator(_, cfg.val_evaluator)\n",
    "setattr(val_eval,'dataset_meta', dataloader.dataset.metainfo)\n",
    "\n",
    "\n",
    "    \n",
    "# eval_results = []\n",
    "for i, data in enumerate(dataloader):\n",
    "    print(f\"Processing image {i}\")\n",
    "    features = model.feature_frontend(data)\n",
    "    result_backend_inference = model.backend_inference(features)\n",
    "    val_eval.process(result_backend_inference)\n",
    "    # eval_result = val_eval.offline_evaluate(result_backend_inference)\n",
    "    # eval_results.append(eval_result)\n",
    "    # result_backend_raw = model.backend_raw(data)\n",
    "    print(f\"Processed image {i}\")\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = val_eval.evaluate(len(dataloader))\n",
    "# result = val_eval.offline_evaluate(result_backend_inference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b048090fc6d3014cd2bdcc9ea88274151058c974d3dd655c4d5f3eddfe0b65cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
