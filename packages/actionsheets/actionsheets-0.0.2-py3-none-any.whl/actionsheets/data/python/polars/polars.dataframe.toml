language = "python"
parent = "python.polars"
name = "dataframe"
title = "Polars DataFrame"
description = "The polars package provides a fast and powerful implementation of data frames"
details = "Some more details here"
code = "import polars as pl"

[create]
section = "Create"
description = "Create a new DataFrame"

[create.dataframes.concat]
what = "From dataframes (concatenate)"
code = "pl.concat([df, df2, dfN])"

[create.dataframes.concat.mix]
what = "From dataframes with different columns"
code = "pl.concat([df, df2, dfN], how = 'diagonal')"

[create.lists]
what = "From column lists"
code = "pl.DataFrame({'A': [1, 2], 'fruits': ['banana', 'apple']})"

[create.list.lists]
what = "From list of lists"
code = "data = [[1, 2, 3], [4, 5, 6]]`<br>`pl.DataFrame(data, schema=['a', 'b', 'c'])"

[create.dict]
what = "From dict"
code = "pl.DataFrame(dict)"

[create.dict.schema]
what = "From dict with schema"
code = "pl.DataFrame(dict, schema = {'col1': pl.Float32, 'col2': pl.Int64, 'col3': pl.Date})"

[create.pandas]
what = "From `pandas.DataFrame`"
code = "pl.from_pandas(data)"

[create.nparray]
what = "From numpy array"
code = """
data = np.array([[1, 2], [3, 4]])\n
pl.DataFrame(data, schema = ['a', 'b'], orient = 'col')
"""

[create.file]
section = "From file formats"

[create.file.csv]
what = "From CSV file"
code = "pl.read_csv('file.csv')"

[prop]
section = "Properties"

[prop.ncol]
what = "Number of columns"
code = "len(data.columns)"

[prop.col.names]
what = "Column names"
code = "data.columns"

[prop.col.types]
what = "Column types"
code = "data.dtypes"

[prop.col.types.map]
what = "Column-types mapping"
code = "data.schema"

[prop.col.which]
what = "Find column index by name"
code = "data.find_idx_by_name('age')"

[prop.nrow]
what = "Number of rows"
code = "data.height"


[test]
section = "Test"

[test.empty]
what = "Empty (no rows)"
code = "data.is_empty()"

[test.col.contains]
what = "Contains column"
code = "'age' in data.columns"

[test.col.contains.multi]
what = "Contains columns"
code = "{'age', 'sex'}.issubset(data.columns)"

[test.col.contains.dyn]
what = "Contains columns _cols_"
code = "set(cols).issubset(data.columns)"

[test.col.missing]
what = "Column is missing"
code = "'age' not in data.columns"

[test.col.equals]
what = "Columns are equal"
code = "?"

[test.col.equal.series]
what = "Columns are equal series"
code = "data['sex'].series_equal(data['sex2'].alias('sex'))"
details = "Series names must match!"

[test.col.missing.any]
what = "Column has missing value"
code = "data['sex'].is_null().any()"

[test.col.missing.none]
what = "Column has no missing values"
code = "data['sex'].is_not_null().all()"

[test.col.duplicate.none]
what = "Column has no duplicate values"
code = "?"

[test.col.duplicate.any]
what = "Column has duplicate values"
code = "?"

[test.col.type]
what = "Column is of type"
code = "data.schema['col1'] == dtype"

[test.col.type.bool]
what = "Column is bool type"
code = "data.schema['alive'] == pl.Bool"

[test.col.type.str]
what = "Column is string type"
code = "data.schema['sex'] == pl.Utf8"

[test.col.type.number]
what = "Columns is numeric"
code = "?"

[test.col.type.int]
what = "Column is integer type"
code = "data.schema['age'] in pl.datatypes.INTEGER_DTYPES"

[test.col.type.int64]
what = "Column is standard integer (64-bit)"
code = "data.schema['age'] == pl.Int64"

[test.col.type.float]
what = "Columns is float"
code = "?"

[test.col.row.gt.all]
what = "Column is consistently rowwise greater than another column"
code = "(data['col1'] > data['col2']).all()"

[test.col.row.gt.any]
what = "Column is sometimes rowwise greater than another column"
code = "(data['col1'] > data['col2']).any()"

[test.rowmask]
section = "Row masking"

[test.rowmask.duplicate]
what = "Duplicated"
code = "data.is_duplicated()"

[test.rowmask.unique]
what = "Unique"
code = "data.is_unique()"


[query]
section = "Query"
description = "Start a lazy query using a LazyFrame by `data.lazy()`. Operations on a LazyFrame are not executed until this is requested by either calling `collect()` or `fetch()`. Lazy operations are advised because they allow for query optimization and more parallelization."

[query.col]
section = "Columns"
description = "Query the row(s) of one or more columns"

[query.col.single]
what = "Single column"
code = "data['col1']"
details = "Short, and works for literals and variables"

[query.col.single.attr]
code = "data.col1"
details = "Shortest"

[query.col.single.select]
code = "data.select('col1')"
details = "Verbose"

[query.col.multi]
what = "Multiple columns"
code = "data.select('col1', 'col2')"

[query.col.multi.var]
what = "Multiple columns, dynamically"
code = "data.select(['col1', 'col2'])"

[query.row]
section = "Rows"

[query.row.empty]
what = "Empty row"
code = "data.clear()"
details = "?"

[query.row.index]
what = "_i_ th row"
code = "data[i]"

[query.row.index.end]
what = "_i_ th row from end"
code = "data[-i]"

[query.row.head]
what = "First _n_ rows (head)"
code = "data.head(n)"

[query.row.tail]
what = "Last _n_ rows (tail)"
code = "data.tail(n)"

[query.row.slice]
what = "Slice of rows from _a_ to _b_"
code = "data[a:b]"

[query.row.slice.fun]
code = "data.slice(a, b)"

[query.row.index.list]
what = "By list of row numbers"
code = "data[rows]"

[query.row.exclude.index]
what = "Exclude the given row numbers"
code = "data.with_row_count().filter(pl.col('row_nr').is_in(rows).not_())"
details = "Leftover row_nr column"

[query.row.exclude.null]
what = "Exclude rows that contain null values"
code = "data.drop_nulls()"

[query.row.exclude.null.col]
what = "Exclude rows that contain null values in certain columns"
code = "data.drop_nulls(['fruits', 'cars'])"

[query.row.cond.col]
what = "Conditionally on column"
code = "data.filter(pl.col('age') >= 18)"

[query.row.cond.cols]
what = "From multiple column conditions"
code = "data.filter((pl.col('age') >= 18) & (pl.col('sex') == 'male'))"

[query.row.limit]
what = "Limit query to first _n_ rows"
code = "data.limit(n)"

[query.row.limit.tail]
what = "Limit query to last _n_ rows"
code = "data.limit(-n)"

[query.row.count.missing]
what = "Number of missing values"
code = "data.null_count()"

[query.row.count.unique.col]
what = "Number of unique values in a column"
code = "data['col1'].n_unique()"

[query.row.count.unique.col.multi]
what = "Number of unique rows across columns"
code = "?"

[query.aggregate]
section = "Aggregate"

[query.aggregate.group]
section = "By group"

[query.aggregate.group.mean]
what = "Mean of column by group"
code = "data.group_by('sex').agg(pl.col('age').mean())"

[query.aggregate.time]
section = "Over time"
description = "Aggregation is done at a fixed datetime interval (e.g., daily, hourly), based on an datetime index."

[query.aggregate.time.row]
section = "Row-based"
description = "Rolling computation for each row (i.e., observation in time)"

[query.aggregate.time.row.solo.right]
what = "Rolling computation for a single column, with right-aligned partial window of max _n_ days and at least _i_ rows"
code = """
data.with_columns(
    pl.col('Hospitalized').rolling_mean(by='Date', closed='both', window_size=f'{n - 1}d', min_periods=i)
)
"""
details = "Currently only available for `rolling_mean()`, not any of the other `rolling_` functions."

[query.aggregate.time.row.right]
what = "Rolling computation with right-aligned partial window of max _n_ days"
code = """
new_data = data.rolling(
    index_column='Date',
    period=f'{n}d'
).agg(
    pl.mean('Obs)
)
"""

[query.aggregate.time.row.right.keep]
what = "Rolling computation with right-aligned partial windows of max _n_ days, keep other columns"
code = """
new_data = data.rolling(
    index_column='Date',
    period=f'{n}d'
).agg(
    pl.exclude(['Date', 'Obs']).last(),
    pl.mean('Obs)
)
"""

[query.aggregate.time.interval]
section = "Interval-based"
description = "Rolling computation for constant (not dynamic!) intervals. This may introduce additional moments in time."

[query.aggregate.time.interval.right]
what = "Daily rolling computation with right-aligned partial window of max _n_ days"
code = """
new_data = data.group_by_dynamic(
    index_column='Date',
    every='1d',
    offset=f'-{n - 1}d',
    period=f'{n - 1}d',
    label='right',
    closed='both',
    start_by='window'
).agg(
    pl.mean('Obs')
)
"""
details = "To be verified"

[query.aggregate.time.interval.left]
what = "Daily rolling computation with left-aligned partial window of max _n_ days"
code = """
new_data = data.group_by_dynamic(
    index_column='Date',
    every='1d',
    offset=f'-{n - 1}d',
    period=f'{n - 1}d',
    label='left',
    closed='both',
    start_by='datapoint'
).agg(
    pl.mean('Obs')
)
"""
details = "To be verified"

[query.aggregate.time.interval.right.prop]
what = "Dynamic daily rolling statistic proportional to number of window observations, with right-aligned partial window of max _n_ days"
code = """
new_data = data.group_by_dynamic(
    index_column='Date',
    every='1d',
    offset=f'-{n - 1}d',
    period=f'{n - 1}d',
    label='right',
    closed='both',
    start_by='window'
).agg(
    pl.sum('Hospitalized').alias('DaysHospitalized'),
    pl.count()
).with_columns(
    Proportion=pl.col('DaysHospitalized') / pl.col('count')
)
"""
details = "To be verified"

[update]
section = "Update"

[update.col.type]
what = "Cast column type"
code = "data.with_columns(pl.col('col1').cast(pl.Float32))"

[update.cols.type]
what = "Cast columns to types"
code = "data.cast({'col1': pl.Float32, 'col2': pl.UInt8})"

[update.col.rename]
what = "Rename column"
code = "data.rename({'old': 'new'})"

[update.col.rename.multi]
what = "Rename columns"
code = "data.rename({'old1': 'new1', 'old2': 'new2'})"

[update.col.order]
what = "Reorder all columns in a given order"
code = "data.select(pl.col(['patient', 'sex', 'age']))"

[update.col.order.some]
what = "Reorder specific columns in a given order"
code = """
cols = ['topic', 'parent', 'language']
data.select(pl.col(cols), pl.exclude(cols))
"""
details = "There doesn't seem to be a shorter way to do this"

[update.col.values]
what = "Update column values"
code = "data.with_columns(pl.col('age') + 5)"

[update.col.values.cond]
what = "Update column values on condition"
code = """
df.with_columns(
    pl.when(pl.col('age') >= 18).
    then(pl.lit(1)).
    otherwise(pl.lit(-1))
)
"""

[update.col.values.conds]
what = "Update column values on conditions"
code = """
df.with_columns(
    pl.when(pl.col('age') >= 18).
    then(pl.lit(1)).
    when(pl.col('Sex') == 'M').
    then(4).
    otherwise(pl.lit(-1))
)
"""

[update.col.values.rowindex]
what = "Update column values for specific rows"
code = """
rows = [1, 3, 5]
data.with_row_count().with_columns(
    pl.when(pl.col('row_nr').is_in(rows)).
    then(pl.lit(True)).
    otherwise(pl.lit(False))
)
"""

[update.fill.null.zero]
what = "Fill nulls with zero"
code = "data.fill_null(strategy = 'zero')"

[update.fill.null.value]
what = "Fill nulls with value"
code = "data.fill_null(value)"

[update.fill.null.locf]
what = "Fill nulls with LOCF"
code = "data.fill_null(strategy='forward')"
details = "Wrong for grouped data"

[update.nan.value]
what = "Fill NaNs with value"
code = "data.fill_nan(value)"

[update.col.replace]
what = "Replace column inplace with series"
code = "data.replace('age', newAgeSeries)"

[update.sort.col]
what = "Sort table by column"
code = "data.sort('col1')"


[grow]
section = "Grow"

[grow.append.col.constant]
what = "Append constant numeric column"
code = "data.with_columns(Intercept=pl.lit(1))"

[grow.append.col.series]
what = "Append column from series"
code = """
s = pl.Series("apple", [10, 20, 30])
data.hstack([s])
"""
details = "Note the brackets"

[grow.append.col.series.inplace]
what = "Append column from series, inplace"
code = "data.hstack(s, in_place = True)"

[grow.insert.col.series.inplace]
what = "Insert column from series, inplace"
code = "data.insert_at_idx(1, s)"

[grow.derive]
section = "Derive columns"

[grow.derive.transform]
what = "Transform another column"
code = "data.with_columns(AgeSq = pl.col('Age') ** 2)"

[grow.derive.transform.multi]
what = "Multiple transformations from another column"
code = """
data.with_columns(
    Age2 = pl.col('Age') ** 2
    Age3 = pl.col('Age') ** 3
)
"""

[grow.derive.mask.pairwise]
what = "Boolean mask based on pairwise comparison of another column"
code = "data.with_columns(CanLive = pl.col('Income') * 3 > pl.col('Rent')"

[grow.derive.values.cond]
what = "Values conditional on another column"
code = """
data.with_columns(
    pl.when(pl.col('age') >= 18).
    then(pl.lit(1)).
    otherwise(pl.lit(-1))
)
"""

[grow.derive.map]
what = "Map another column"
code = """
map = dict(1 = 'a', 2 = 'b', 3 = 'c')
data.with_columns(
    NumCat = pl.col('Num').map_dict(map).cast(pl.Categorical)
)
"""

[grow.derive.str.datetime]
what = "Parse string column to date"
code = "data.with_columns(date=pl.col('datestring').str.to_date())"

[grow.derive.str.datetime.format]
what = "Parse string column to date with known format"
code = "data.with_columns(date = pl.col('datestring').str.to_date('%Y-%m-%d'))"

[grow.derive.datetime.weekday]
what = "Weekday from date column"
code = "?"

[grow.derive.datetime.month]
what = "Month from date column"
code = "data.with_columns(month = pl.col('date').dt.month())"

[grow.derive.datetime.year]
what = "Year from date column"
code = "data.with_columns(year = pl.col('Date').dt.year())"

[grow.derive.datetime.epoch.seconds]
what = "Integer seconds since Unix epoch to datetime"
code = "data.with_columns(date=pl.from_epoch(pl.col('time')))"

[grow.derive.datetime.epoch.days]
what = "Integer daystamp since Unix epoch to date"
code = "data.with_columns(date=pl.from_epoch(pl.col('time'), time_unit='d'))"

[grow.derive.datetime.epoch.ms]
what = "Integer milliseconds timestamp since Unix epoch to datetime"
code = "data.with_columns(date=pl.from_epoch(pl.col('time'), time_unit='ms'))"

[grow.derive.agg.group]
what = "Group-wise from aggregate value"
code = """
data.with_columns(
    DaysSinceStart = pl.col('Date') - pl.col('Date').min().over('Subject').cast(pl.Int) + 1
)
"""

[grow.row]
section = "Add rows"

[grow.row.tuple]
what = "Add row as tuple"
code = "?"

[grow.row.list.tuple]
what = "Add list of tuples"
code = "?"

[grow.row.dataframe]
what = "Add data frame"
code = "pl.concat(data, df2)"

[grow.row.dataframe.inplace]
what = "Add data frame inplace"
code = "data.extend(data2)"

[grow.row.dataframes.inplace]
what = "Add data frames inplace"
code = """
data.vstack(data2)
data.vstack(dataN)
data.rechunk()
"""

[shrink]
section = "Shrink the data frame"


[shrink.col]
section = "Remove columns"

[shrink.col.single]
what = "Remove column"
code = "data.drop('Age')"

[shrink.col.multi]
what = "Remove columns"
code = "data.drop(['Age', 'Sex'])"

[shrink.col.multi.select]
code = "data.select(pl.exclude(['Age', 'Sex']))"

[shrink.col.multi.inplace]
what = "Remove column inplace"
code = "data.drop_in_place('Age')"
details = "Returns the dropped column"

[shrink.col.keep.single]
what = "Keep a single column"
code = "data.select(pl.col('Age'))"

[shrink.col.keep.multi]
what = "Keep multiple columns (column subset)"
code = "data.select(pl.col(['Age', 'Sex'])"

[shrink.col.numeric]
what = "Remove all numeric columns"
code = "data.drop(cs.numeric())"


[shrink.row]
section = "Remove rows"

[shrink.row.index]
what = "Remove rows by row numbers"
code = "data.filter(~pl.arange(0, pl.count()).is_in([1, 5, 7]))"

[shrink.row.index.r]
code = "data.with_row_index().filter(~pl.col('index').is_in([1, 5, 7]))"
details = "Output contains additional column 'Index'"

[shrink.row.index.except]
what = "Remove all rows except the given row numbers"
code = "data[[1, 6]]"

[shrink.row.index.except.filter]
code = "data.filter(pl.arange(0, pl.count()).is_in([1, 5, 7]))"


[reshape]
section = "Reshape"

[reshape.long]
what = "From wide to long format"
code = "data.melt(id_vars='sex', value_vars=['a', 'b'])"

[reshape.narrow]
what = "To narrow format"
code = "data.explode(?)"
details = "?"

[merge]
section = "Merge two data frames"

[merge.sorted]
what = "Merge two data frames on the sorted key"
code = "data.merge(data2)"

[merge.inner]
what = "Inner join"
code = "data.join(data2, on = ['sex', 'country'])"

[merge.left]
what = "Left join"
code = "data.join(data2, on = ['sex', 'country'], how = 'left')"

[merge.right]
what = "Right join"
code = "data.join(data2, on = ['sex', 'country'], how = 'right')"

[merge.outer]
what = "Outer join"
code = "data.join(data2, on = ['sex', 'country'], how = 'outer')"

[merge.cross]
what = "Cross join"
code = "data.join(data2, on = ['sex', 'country'], how = 'cross')"

[merge.semi]
what = "Semi join (one match per index)"
code = "data.join(data2, on = ['sex', 'country'], how = 'semi')"

[merge.anti]
what = "Anti join (exclude matches from table 2)"
code = "data.join(data2, on = ['sex', 'country'], how = 'anti')"


[extract]
section = "Extract"

[extract.col.series]
what = "Get column as series"
code = "data['col1']"

[extract.col.list]
what = "Get column as list" 
code = "list(data['col1'])"
details = "?"

[extract.row.tuple]
what = "Get _i_ th row as tuple"
code = "data.row(i)"

[extract.rows.list.tuple]
what = "Get rows as list of tuple"
code = "data.rows(...)"
details = "?"

[extract.cell.first]
what = "First item (cell)"
code = "data.item(0, 0)"

[extract.cell.at]
what = "Item (cell) from row _i_ and column index _j_"
code = "data.item(i, j)"

[extract.cell.at.col]
what = "Item (cell) from row _i_ and column name _name_"
code = "data.item(i, name)"


[convert]
section = "Convert"

[convert.pandas]
what = "To `pandas.DataFrame`"
code = "data.to_pandas()"

[convert.list.series]
what = "To list of series"
code = "data.get_columns()"

[convert.split.dataframes.list.col]
what = "Split into list of data frames based on column"
code = "data.partition_by('sex')"

[convert.split.dataframes.list.cols]
what = "Split into list of data frames based on column tuples"
code = "data.partition_by('sex', 'country')"

[convert.split.dataframes.dict]
what = "Split into dict of data frames based on column(s)"
code = "data.partition_by('sex', 'country', as_dict = True)"

[convert.format.csv]
what = "To CSV file"
code = "data.write_csv('derp.csv')"

[convert.format.parquet]
what = "To Parquet file"
code = "data.write_parquet('derp.parquet')"

[convert.format.json]
what = "To JSON"
code = "?"
