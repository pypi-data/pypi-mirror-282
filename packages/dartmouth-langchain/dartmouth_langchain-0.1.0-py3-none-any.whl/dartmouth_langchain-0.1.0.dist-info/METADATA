Metadata-Version: 2.1
Name: dartmouth_langchain
Version: 0.1.0
Summary: LangChain components for Dartmouth-hosted models.
Author-email: Simon Stone <simon.stone@dartmouth.edu>
Project-URL: Homepage, https://git.dartmouth.edu/lib-digital-strategies/RDS/projects/dartmouth-langchain
Project-URL: Issues, https://git.dartmouth.edu/lib-digital-strategies/RDS/projects/dartmouth-langchain/-/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: dartmouth-auth
Requires-Dist: langchain
Requires-Dist: langchain-community
Requires-Dist: langchain-huggingface
Requires-Dist: text-generation
Requires-Dist: python-dotenv

# Dartmouth LangChain

LangChain components for Dartmouth-hosted models.

## Getting started

1. Install the package:

```
pip install dartmouth-langchain
```

2. Obtain a Dartmouth API key from [developer.dartmouth.edu](https://developer.dartmouth.edu/)
3. Store the API key as an environment variable called `DARTMOUTH_API_KEY`:
```
export DARTMOUTH_API_KEY=<your_key_here>
```

## Using the library

Using a Dartmouth-hosted chat model:

```{python}
from dartmouth_langchain import DartmouthChatModel


llm = DartmouthChatModel()

llm.invoke("<s>[INST] Hi there! [/INST]")
```
> **Note:** Many chat models require the prompts to have a particular formatting to work correctly! The default model is a chat model from the Llama 2 family and thus [requires the tags shown in the example](https://gpus.llm-utils.org/llama-2-prompt-template/) above.

Using a Dartmouth-hosted embeddings model:

```{python}
from dartmouth_langchain import DartmouthEmbeddingsModel


embeddings = DartmouthEmbeddingsModel()

embeddings.embed_query("Hello? Is there anybody in there?")
```

