{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating GNN layers\n",
    "\n",
    "One of the primary advantage of this library is the fact that GNN layers are independent from model architecture, thus allowing more flexibility with the code by easily swapping different layer types as hyper-parameters. However, this requires that the layers be implemented using the PyG library, and must be inherited from the class [`BaseGraphStructure`](https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure), which standardizes the inputs, outputs and properties of the layers. Thus, the architecture can be handled independantly using the class [`FeedForwardPyg`](https://graphium-docs.datamol.io/stable/api/graphium.nn/architectures.html#graphium.nn.architectures.pyg_architectures.FeedForwardPyg) or other custom classes.\n",
    "\n",
    "We will first start by a simple layer that does not use edges, to a more complex layer that uses edges.\n",
    "\n",
    "Since these examples are built on top of PyG, we recommend looking at their [Docs](https://pytorch-geometric.readthedocs.io/en/latest/) and [Tutorials](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html) for more info. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of content\n",
    "1. [Define test graph](#Define-synthetic-test-graph)\n",
    "2. [Create simple layer](#Creating-a-simple-layer)\n",
    "3. [Test simple layer](#Test-our-simple-layer)\n",
    "4. [Create complex layer](#Creating-a-complex-layer-with-edges)\n",
    "5. [Test complex layer](#Test-our-complex-layer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.typing import (\n",
    "    OptPairTensor,\n",
    "    OptTensor\n",
    ")\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Callable, Union, Optional, List\n",
    "from graphium.nn.base_graph_layer import BaseGraphStructure\n",
    "from graphium.nn.base_layers import FCLayer\n",
    "from graphium.utils.decorators import classproperty\n",
    "\n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define synthetic test graph\n",
    "\n",
    "We define below a small batched graph on which we can test the created layers. You can also use synthetic graphs to define unit tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 7], feat=[7, 5], edge_feat=[7, 13], batch=[7], ptr=[3])\n"
     ]
    }
   ],
   "source": [
    "in_dim = 5          # Input node-feature dimensions\n",
    "out_dim = 11        # Desired output node-feature dimensions\n",
    "in_dim_edges = 13   # Input edge-feature dimensions\n",
    "\n",
    "\n",
    "# Let's create 2 simple pyg graphs. \n",
    "# start by specifying the edges with edge index\n",
    "edge_idx1 = torch.tensor([[0, 1, 2],\n",
    "                          [1, 2, 3]])\n",
    "edge_idx2 = torch.tensor([[2, 0, 0, 1],\n",
    "                          [0, 1, 2, 0]])\n",
    "\n",
    "# specify the node features, convention with variable x\n",
    "x1 = torch.randn(edge_idx1.max() + 1, in_dim, dtype=torch.float32)\n",
    "x2 = torch.randn(edge_idx2.max() + 1, in_dim, dtype=torch.float32)\n",
    "\n",
    "# specify the edge features in e\n",
    "e1 = torch.randn(edge_idx1.shape[-1], in_dim_edges, dtype=torch.float32)\n",
    "e2 = torch.randn(edge_idx2.shape[-1], in_dim_edges, dtype=torch.float32)\n",
    "\n",
    "# make the pyg graph objects with our constructed features\n",
    "g1 = Data(feat=x1, edge_index=edge_idx1, edge_feat=e1)\n",
    "g2 = Data(feat=x2, edge_index=edge_idx2, edge_feat=e2)\n",
    "\n",
    "# put the two graphs into a Batch graph\n",
    "bg = Batch.from_data_list([g1, g2])\n",
    "\n",
    "# The batched graph will show as a single graph with 7 nodes\n",
    "print(bg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple layer\n",
    "\n",
    "Here, we will show how to create a GNN layer that simples does a mean aggregation on the neighbouring features.\n",
    "\n",
    "First, for the layer to be fully compatible with the flexible architecture provided by [`FeedForwardPyg`](https://graphium-docs.datamol.io/stable/api/graphium.nn/architectures.html#graphium.nn.architectures.pyg_architectures.FeedForwardPyg), it needs to inherit from the class [`BaseGraphStructure`](https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure). This base-layer has multiple virtual methods that must be implemented in any class that inherits from it.\n",
    "\n",
    "The virtual methods are below\n",
    "\n",
    "- [`layer_supports_edges`](https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure): We want to return `False` since our layer doesn't support edges\n",
    "- [`layer_inputs_edges`](https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.layer_inputs_edges): We want to return `False` since our layer doesn't input edges\n",
    "- [`layer_outputs_edges`](https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.layer_outputs_edges): We want to return `False` since our layer doesn't output edges\n",
    "- [`out_dim_factor`](https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.out_dim_factor): We want to return `1` since the output dimension does not depend on internal parameters.\n",
    "\n",
    "The example is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from message passing class from pyg \n",
    "# inherit from BaseGraphStructure\n",
    "# this example is also based of : https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/simple_conv.html#SimpleConv.forward\n",
    "\n",
    "class SimpleMeanLayer(MessagePassing, BaseGraphStructure):\n",
    "    def __init__(self, \n",
    "                 in_dim: int, \n",
    "                 out_dim: int, \n",
    "                 activation: Union[Callable, str] = \"relu\", \n",
    "                 dropout: float = 0.0, \n",
    "                 normalization: Union[str, Callable] = \"none\",\n",
    "                 aggr: str = \"mean\",\n",
    "                ):\n",
    "        r\"\"\"\n",
    "        add documentation in the format shown here for this layer to be automatically added to the graphium api reference\n",
    "        the type information will also be automatically shown in the doc\n",
    "        \n",
    "        Parameters:\n",
    "\n",
    "            in_dim:\n",
    "                Input feature dimensions of the layer\n",
    "\n",
    "            out_dim:\n",
    "                Output feature dimensions of the layer\n",
    "\n",
    "            activation:\n",
    "                activation function to use in the layer\n",
    "\n",
    "            dropout:\n",
    "                The ratio of units to dropout. Must be between 0 and 1\n",
    "\n",
    "            normalization:\n",
    "                Normalization to use. Choices:\n",
    "\n",
    "                - \"none\" or `None`: No normalization\n",
    "                - \"batch_norm\": Batch normalization\n",
    "                - \"layer_norm\": Layer normalization\n",
    "                - `Callable`: Any callable function\n",
    "            aggr:\n",
    "                what aggregation to use (\"add\", \"mean\" or \"max\")\n",
    "        \"\"\"\n",
    "        # Initialize the parent class\n",
    "        MessagePassing.__init__(self, node_dim=0, aggr=aggr)\n",
    "        BaseGraphStructure.__init__(self,\n",
    "                                    in_dim=in_dim, \n",
    "                                    out_dim=out_dim, \n",
    "                                    activation=activation,\n",
    "                                    dropout=dropout, \n",
    "                                    normalization=normalization)\n",
    "        \n",
    "        self.aggr = aggr\n",
    "        self._initialize_activation_dropout_norm()\n",
    "        # Create the mlp layer \n",
    "        # https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.MLP\n",
    "        self.transform = FCLayer(in_dim=in_dim, out_dim=out_dim)\n",
    "            \n",
    "    # define the forward function \n",
    "    def forward(self, \n",
    "                batch: Union[Data, Batch],\n",
    "               ) -> Union[Data, Batch]:\n",
    "        r\"\"\"\n",
    "        similarly add documentation to the functions in the class\n",
    "        \n",
    "        Parameters:\n",
    "            batch: pyg Batch graphs to pass through the layer\n",
    "        Returns:\n",
    "            batch: pyg Batch graphs\n",
    "        \"\"\"\n",
    "        \n",
    "        x = batch.feat\n",
    "        edge_index = batch.edge_index\n",
    "        \n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        # propagate_type: (x: OptPairTensor, edge_weight: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x)\n",
    "        out = self.transform(out)\n",
    "        out = self.apply_norm_activation_dropout(out, batch_idx=batch.batch)\n",
    "        batch.feat = out\n",
    "        return batch\n",
    "    \n",
    "    def message(self, x_j):\n",
    "\n",
    "        return x_j\n",
    "\n",
    "    # Finally, we define all the virtual properties according to how the class works with proper documentation of course\n",
    "    @classproperty\n",
    "    def layer_supports_edges(cls) -> bool:\n",
    "        r\"\"\"\n",
    "        Return a boolean specifying if the layer type supports edges or not.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            bool:\n",
    "                False for the current class\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def layer_inputs_edges(self) -> bool:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "            bool:\n",
    "                Returns False\n",
    "        \"\"\"\n",
    "        return False\n",
    "            \n",
    "    @property\n",
    "    def layer_outputs_edges(self) -> bool:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "            bool:\n",
    "                Always ``False`` for the current class\n",
    "        \"\"\"\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def out_dim_factor(self) -> int:\n",
    "        r\"\"\"\n",
    "        Get the factor by which the output dimension is multiplied for\n",
    "        the next layer.\n",
    "\n",
    "        For standard layers, this will return ``1``.\n",
    "        Returns:\n",
    "\n",
    "            int:\n",
    "                Always ``1`` for the current class\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our simple layer\n",
    "\n",
    "Now, we are ready to test the `SimpleMeanLayer` on our constructed graphs. Note that in this example, we **ignore** the edge features since they are not supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5])\n",
      "torch.Size([7, 11])\n"
     ]
    }
   ],
   "source": [
    "graph = deepcopy(bg)\n",
    "print(graph.feat.shape)\n",
    "\n",
    "layer = SimpleMeanLayer(\n",
    "            in_dim=in_dim, \n",
    "            out_dim=out_dim, \n",
    "            activation=\"relu\", \n",
    "            dropout=.3, \n",
    "            normalization=\"batch_norm\")\n",
    "\n",
    "graph = layer(graph)\n",
    "print(graph.feat.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a complex layer with edges\n",
    "\n",
    "Here, we will show how to create a GNN layer that does a mean aggregation on the neighbouring features, concatenated to the edge features with their neighbours. In that case, only the node features will change, and the network will not update the edge features.\n",
    "\n",
    "The virtual methods will have different outputs\n",
    "\n",
    "- [`layer_supports_edges`](https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure): We want to return `True` since our layer does support edges\n",
    "- [`layer_inputs_edges`](https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.layer_inputs_edges): We want to return `True` since our layer does input edges\n",
    "- [`layer_outputs_edges`](https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.layer_outputs_edges): We want to return `False` since our layer will not output new edges\n",
    "- [`out_dim_factor`](https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_graph_layer.BaseGraphStructure.out_dim_factor): We want to return `1` since the output dimension does not depend on internal parameters.\n",
    "\n",
    "The example is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherent from message passing class from pyg \n",
    "# inherent from BaseGraphStructure\n",
    "# adapting example from graphium/nn/pyg_layers/png_pyg.py\n",
    "\n",
    "class ComplexMeanLayer(MessagePassing, BaseGraphStructure):\n",
    "    def __init__(self, \n",
    "                 in_dim: int, \n",
    "                 out_dim: int, \n",
    "                 in_dim_edges: int, \n",
    "                 activation: Union[Callable, str] = \"relu\", \n",
    "                 dropout: float = 0.0, \n",
    "                 normalization: Union[str, Callable] = \"none\",\n",
    "                 aggr: str = \"mean\",\n",
    "                ):\n",
    "        r\"\"\"\n",
    "        add documentation in the format shown here for this layer to be automatically added to the graphium api reference\n",
    "        the type information will also be automatically shown in the doc\n",
    "        \n",
    "        Parameters:\n",
    "\n",
    "            in_dim:\n",
    "                Input feature dimensions of the layer\n",
    "\n",
    "            out_dim:\n",
    "                Output feature dimensions of the layer\n",
    "            \n",
    "            in_dim_edges:\n",
    "                Input edge feature dimension of the layer\n",
    "\n",
    "            activation:\n",
    "                activation function to use in the layer\n",
    "\n",
    "            dropout:\n",
    "                The ratio of units to dropout. Must be between 0 and 1\n",
    "\n",
    "            normalization:\n",
    "                Normalization to use. Choices:\n",
    "\n",
    "                - \"none\" or `None`: No normalization\n",
    "                - \"batch_norm\": Batch normalization\n",
    "                - \"layer_norm\": Layer normalization\n",
    "                - `Callable`: Any callable function\n",
    "            aggr:\n",
    "                what aggregation to use (\"add\", \"mean\" or \"max\")\n",
    "        \"\"\"\n",
    "        # Initialize the parent class\n",
    "        MessagePassing.__init__(self, node_dim=0, aggr=aggr)\n",
    "        BaseGraphStructure.__init__(self,\n",
    "                                    in_dim=in_dim, \n",
    "                                    out_dim=out_dim, \n",
    "                                    activation=activation,\n",
    "                                    dropout=dropout, \n",
    "                                    normalization=normalization)\n",
    "        \n",
    "        self.aggr = aggr\n",
    "        self._initialize_activation_dropout_norm()\n",
    "        # Create the mlp layer \n",
    "        # https://graphium-docs.datamol.io/stable/api/graphium.nn/graphium.nn.html#graphium.nn.base_layers.MLP\n",
    "        self.transform = FCLayer(in_dim=(in_dim + in_dim_edges), out_dim=out_dim)\n",
    "            \n",
    "    # define the forward function \n",
    "    def forward(self, \n",
    "                batch: Union[Data, Batch],\n",
    "               ) -> Union[Data, Batch]:\n",
    "        r\"\"\"\n",
    "        similarly add documentation to the functions in the class\n",
    "        \n",
    "        Parameters:\n",
    "            batch: pyg Batch graphs to pass through the layer\n",
    "        Returns:\n",
    "            batch: pyg Batch graphs\n",
    "        \"\"\"        \n",
    "        x = batch.feat\n",
    "        edge_index = batch.edge_index\n",
    "        edge_feat = batch.edge_feat\n",
    "        \n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        # propagate_type: (x: OptPairTensor, edge_weight: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x, edge_feat=edge_feat, size=None)\n",
    "        out = self.transform(out)\n",
    "        out = self.apply_norm_activation_dropout(out, batch_idx=batch.batch)\n",
    "        batch.feat = out\n",
    "        return batch\n",
    "    \n",
    "    def message(self, x_j: Tensor, edge_feat: OptTensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        message function\n",
    "\n",
    "        Parameters:\n",
    "            x_i: node features\n",
    "            x_j: neighbour node features\n",
    "            edge_feat: edge features\n",
    "        Returns:\n",
    "            feat: the message\n",
    "        \"\"\"\n",
    "        feat = torch.cat([x_j, edge_feat], dim=-1)\n",
    "        return feat\n",
    "    \n",
    "    def aggregate(\n",
    "        self,\n",
    "        inputs: Tensor,\n",
    "        index: Tensor,\n",
    "        edge_index: Tensor,\n",
    "        dim_size: Optional[int] = None,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"\n",
    "        aggregate function\n",
    "        Parameters:\n",
    "            inputs: input features\n",
    "            index: index of the nodes\n",
    "            edge_index: edge index\n",
    "            dim_size: dimension size\n",
    "        Returns:\n",
    "            out: aggregated features\n",
    "        \"\"\"\n",
    "        out = scatter(inputs, index, 0, None, dim_size, reduce=self.aggr)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    # Finally, we define all the virtual properties according to how the class works with proper documentation of course\n",
    "    @classproperty\n",
    "    def layer_supports_edges(cls) -> bool:\n",
    "        r\"\"\"\n",
    "        Return a boolean specifying if the layer type supports edges or not.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            bool:\n",
    "                False for the current class\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def layer_inputs_edges(self) -> bool:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "            bool:\n",
    "                Returns True\n",
    "        \"\"\"\n",
    "        return True\n",
    "            \n",
    "    @property\n",
    "    def layer_outputs_edges(self) -> bool:\n",
    "        r\"\"\"\n",
    "        Returns:\n",
    "\n",
    "            bool:\n",
    "                Always ``True`` for the current class\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    @property\n",
    "    def out_dim_factor(self) -> int:\n",
    "        r\"\"\"\n",
    "        Get the factor by which the output dimension is multiplied for\n",
    "        the next layer.\n",
    "\n",
    "        For standard layers, this will return ``1``.\n",
    "        Returns:\n",
    "\n",
    "            int:\n",
    "                Always ``1`` for the current class\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test our complex layer \n",
    "\n",
    "Now, we are ready to test the `ComplexMeanLayer` on our constructed graphs. Note that in this example, we utilize the edge features and node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5])\n",
      "torch.Size([7, 13])\n",
      "torch.Size([7, 11])\n"
     ]
    }
   ],
   "source": [
    "graph = deepcopy(bg)\n",
    "print(graph.feat.shape)\n",
    "print(graph.edge_feat.shape)\n",
    "\n",
    "layer = ComplexMeanLayer(\n",
    "            in_dim=in_dim, \n",
    "            out_dim=out_dim, \n",
    "            in_dim_edges=in_dim_edges,\n",
    "            activation=\"relu\", \n",
    "            dropout=.3, \n",
    "            normalization=\"batch_norm\")\n",
    "\n",
    "graph = layer(graph)\n",
    "print(graph.feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4a99d018a205fcbcc0480c84566beaebcb91b08d0414b39a842df533e2a1d25"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
