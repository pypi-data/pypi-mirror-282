{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making GNN Networks\n",
    "\n",
    "In this example, you will learn how to easily build a full multi-task GNN using any kind of GNN layer. This tutorial uses the architecture defined by the class `FullGraphMultiTaskNetwork`.\n",
    "\n",
    "`FullGraphMultiTaskNetwork` is an architecture that takes as input node features and (optionally) edge features. It applies a pre-MLP on both sets of features, then passes them into a main GNN network, and finally applies graph output NNs to produces the final outputs on possibly several task levels (graph, node, or edge-level).\n",
    "\n",
    "The network is very easy to built via a dictionnary of parameter that allow to customize each part of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "from graphium.nn.architectures import FullGraphMultiTaskNetwork\n",
    "\n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create some simple batched graphs that will be used accross the examples. Here, `bg` is a batch containing 2 graphs with random node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 7], feat=[7, 5], edge_feat=[7, 13], batch=[7], ptr=[3])\n"
     ]
    }
   ],
   "source": [
    "in_dim = 5          # Input node-feature dimensions\n",
    "in_dim_edges = 13   # Input edge-feature dimensions\n",
    "out_dim = 11        # Desired output node-feature dimensions\n",
    "\n",
    "\n",
    "# Let's create 2 simple pyg graphs. \n",
    "# start by specifying the edges with edge index\n",
    "edge_idx1 = torch.tensor([[0, 1, 2],\n",
    "                          [1, 2, 3]])\n",
    "edge_idx2 = torch.tensor([[2, 0, 0, 1],\n",
    "                          [0, 1, 2, 0]])\n",
    "\n",
    "# specify the node features, convention with variable x\n",
    "x1 = torch.randn(edge_idx1.max() + 1, in_dim, dtype=torch.float32)\n",
    "x2 = torch.randn(edge_idx2.max() + 1, in_dim, dtype=torch.float32)\n",
    "\n",
    "# specify the edge features in e\n",
    "e1 = torch.randn(edge_idx1.shape[-1], in_dim_edges, dtype=torch.float32)\n",
    "e2 = torch.randn(edge_idx2.shape[-1], in_dim_edges, dtype=torch.float32)\n",
    "\n",
    "# make the pyg graph objects with our constructed features\n",
    "g1 = Data(feat=x1, edge_index=edge_idx1, edge_feat=e1)\n",
    "g2 = Data(feat=x2, edge_index=edge_idx2, edge_feat=e2)\n",
    "\n",
    "# put the two graphs into a Batch graph\n",
    "bg = Batch.from_data_list([g1, g2])\n",
    "\n",
    "# The batched graph will show as a single graph with 7 nodes\n",
    "print(bg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a network\n",
    "\n",
    "To build the network, we must define the arguments to pass at the different steps:\n",
    "\n",
    "- `pre_nn_kwargs`: The parameters used by a feed-forward neural network on the input node-features, before passing to the convolutional layers. See class `FeedForwardNN` for details on the required parameters. Will be ignored if set to `None`.\n",
    "\n",
    "- `gnn_kwargs`: The parameters used by a feed-forward **graph** neural network on the features after it has passed through the pre-processing NN. See class `FeedForwardGraph` for details on the required parameters.\n",
    "\n",
    "- `task_heads_kwargs`: The parameters used to specify the different task heads for possibly multiple tasks, each with a specified `task_level` (graph, node or edge).\n",
    "\n",
    "- `graph_output_nn_kwargs`: The parameters used by the graph output NNs to process the features after the GNN layers. We need to to specify a NN for each `task_level` occuring in in the `task_heads_kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dim_1 = 23\n",
    "temp_dim_2 = 17\n",
    "\n",
    "pre_nn_kwargs = {\n",
    "    \"in_dim\": in_dim,\n",
    "    \"out_dim\": temp_dim_1,\n",
    "    \"hidden_dims\": 4,\n",
    "    \"depth\": 2,\n",
    "    \"activation\": 'relu',\n",
    "    \"last_activation\": \"none\",\n",
    "    \"dropout\": 0.2\n",
    "}\n",
    "\n",
    "gnn_kwargs = {\n",
    "    \"in_dim\": temp_dim_1,\n",
    "    \"out_dim\": temp_dim_2,\n",
    "    \"hidden_dims\": 5,\n",
    "    \"depth\": 1,\n",
    "    \"activation\": 'gelu',\n",
    "    \"last_activation\": None,\n",
    "    \"dropout\": 0.1,\n",
    "    \"normalization\": 'layer_norm',\n",
    "    \"last_normalization\": 'layer_norm',\n",
    "    \"residual_type\": 'simple',\n",
    "    \"virtual_node\": None,\n",
    "    \"layer_type\": 'pyg:gcn',\n",
    "    \"layer_kwargs\": None\n",
    "}\n",
    "\n",
    "task_heads_kwargs = {\n",
    "    \"graph-task-1\": {\n",
    "        \"task_level\": 'graph',\n",
    "        \"out_dim\": 3,\n",
    "        \"hidden_dims\": 32,\n",
    "        \"depth\": 2,\n",
    "        \"activation\": 'relu',\n",
    "        \"last_activation\": None,\n",
    "        \"dropout\": 0.1,\n",
    "        \"normalization\": None,\n",
    "        \"last_normalization\": None,\n",
    "        \"residual_type\": \"none\"\n",
    "    },\n",
    "    \"graph-task-2\": {\n",
    "        \"task_level\": 'graph',\n",
    "        \"out_dim\": 4,\n",
    "        \"hidden_dims\": 32,\n",
    "        \"depth\": 2,\n",
    "        \"activation\": 'relu',\n",
    "        \"last_activation\": None,\n",
    "        \"dropout\": 0.1,\n",
    "        \"normalization\": None,\n",
    "        \"last_normalization\": None,\n",
    "        \"residual_type\": \"none\"\n",
    "    },\n",
    "    \"node-task-1\": {\n",
    "        \"task_level\": 'node',\n",
    "        \"out_dim\": 2,\n",
    "        \"hidden_dims\": 32,\n",
    "        \"depth\": 2,\n",
    "        \"activation\": 'relu',\n",
    "        \"last_activation\": None,\n",
    "        \"dropout\": 0.1,\n",
    "        \"normalization\": None,\n",
    "        \"last_normalization\": None,\n",
    "        \"residual_type\": \"none\"\n",
    "    }\n",
    "}\n",
    "\n",
    "graph_output_nn_kwargs = {\n",
    "    \"graph\": {\n",
    "        \"pooling\": ['sum'],\n",
    "        \"out_dim\": temp_dim_2,\n",
    "        \"hidden_dims\": temp_dim_2,\n",
    "        \"depth\": 1,\n",
    "        \"activation\": 'relu',\n",
    "        \"last_activation\": None,\n",
    "        \"dropout\": 0.1,\n",
    "        \"normalization\": None,\n",
    "        \"last_normalization\": None,\n",
    "        \"residual_type\": \"none\"\n",
    "    },\n",
    "    \"node\": {\n",
    "        \"pooling\": None,\n",
    "        \"out_dim\": temp_dim_2,\n",
    "        \"hidden_dims\": temp_dim_2,\n",
    "        \"depth\": 1,\n",
    "        \"activation\": 'relu',\n",
    "        \"last_activation\": None,\n",
    "        \"dropout\": 0.1,\n",
    "        \"normalization\": None,\n",
    "        \"last_normalization\": None,\n",
    "        \"residual_type\": \"none\"\n",
    "    }\n",
    "}\n",
    "    \n",
    "\n",
    "gnn_net = FullGraphMultiTaskNetwork(\n",
    "    gnn_kwargs=gnn_kwargs,\n",
    "    pre_nn_kwargs=pre_nn_kwargs, \n",
    "    task_heads_kwargs=task_heads_kwargs,\n",
    "    graph_output_nn_kwargs = graph_output_nn_kwargs\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the network\n",
    "\n",
    "Once the network is defined, we only need to run the forward pass on the input graphs to get a prediction.\n",
    "\n",
    "The model outputs a dictionary of outputs, one for each task specified in `task_heads_kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5])\n",
      "\n",
      "\n",
      "FullGNN\n",
      "---------\n",
      "    pre-NN(depth=2, ResidualConnectionNone)\n",
      "        [FCLayer[5 -> 4 -> 23]\n",
      "    \n",
      "    GNN(depth=1, ResidualConnectionSimple(skip_steps=1))\n",
      "        GCNConvPyg[23 -> 17]\n",
      "        \n",
      "    \n",
      "        Task heads:\n",
      "        graph-task-1: NN-graph-task-1(depth=2, ResidualConnectionNone)\n",
      "            [FCLayer[17 -> 32 -> 3]\n",
      "        graph-task-2: NN-graph-task-2(depth=2, ResidualConnectionNone)\n",
      "            [FCLayer[17 -> 32 -> 4]\n",
      "        node-task-1: NN-node-task-1(depth=2, ResidualConnectionNone)\n",
      "            [FCLayer[17 -> 32 -> 2]\n",
      "\n",
      "\n",
      "graph-task-1 torch.Size([2, 3])\n",
      "graph-task-2 torch.Size([2, 4])\n",
      "node-task-1 torch.Size([7, 2])\n"
     ]
    }
   ],
   "source": [
    "graph = deepcopy(bg)\n",
    "print(graph.feat.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(gnn_net)\n",
    "print(\"\\n\")\n",
    "\n",
    "out = gnn_net(graph)\n",
    "for task in out.keys():\n",
    "    print(task, out[task].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a network (using additional edge features)\n",
    "\n",
    "We can further use a GNN that uses additional edge features as inputs. For that, we only need to slightly modify the `gnn_kwargs` from above. Below, we define the new `gnn_edge_kwargs`, where we can set the `layer_type` to one of the GNN layers that support edge features (e.g., `pyg:gine` or `pyg:gps`) and add the additional parameter `in_dim_edges`. Optionally, we can configure a preprocessing NN for the the edge features via a dictionaly `pre_nn_edges_kwargs` analogous to `pre_nn_kwargs` above, or skip this step by setting it to `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dim_edges = 8\n",
    "\n",
    "gnn_edge_kwargs = {\n",
    "    \"in_dim\": temp_dim_1,\n",
    "    \"in_dim_edges\": temp_dim_edges,\n",
    "    \"out_dim\": temp_dim_2,\n",
    "    \"hidden_dims\": 5,\n",
    "    \"depth\": 1,\n",
    "    \"activation\": 'gelu',\n",
    "    \"last_activation\": None,\n",
    "    \"dropout\": 0.1,\n",
    "    \"normalization\": 'layer_norm',\n",
    "    \"last_normalization\": 'layer_norm',\n",
    "    \"residual_type\": 'simple',\n",
    "    \"virtual_node\": None,\n",
    "    \"layer_type\": 'pyg:gine',\n",
    "    \"layer_kwargs\": None\n",
    "}\n",
    "\n",
    "pre_nn_edges_kwargs = {\n",
    "    \"in_dim\": in_dim_edges,\n",
    "    \"out_dim\": temp_dim_edges,\n",
    "    \"hidden_dims\": 4,\n",
    "    \"depth\": 2,\n",
    "    \"activation\": 'relu',\n",
    "    \"last_activation\": \"none\",\n",
    "    \"dropout\": 0.2\n",
    "}\n",
    "\n",
    "\n",
    "gnn_net_edges = FullGraphMultiTaskNetwork(\n",
    "    gnn_kwargs=gnn_edge_kwargs,\n",
    "    pre_nn_kwargs=pre_nn_kwargs,\n",
    "    pre_nn_edges_kwargs=pre_nn_edges_kwargs,\n",
    "    task_heads_kwargs=task_heads_kwargs,\n",
    "    graph_output_nn_kwargs = graph_output_nn_kwargs\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the network (using additional edge features)\n",
    "\n",
    "Again, we only need to run the forward pass on the input graphs to get a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5])\n",
      "torch.Size([7, 13])\n",
      "\n",
      "\n",
      "FullGNN\n",
      "---------\n",
      "    pre-NN(depth=2, ResidualConnectionNone)\n",
      "        [FCLayer[5 -> 4 -> 23]\n",
      "    \n",
      "    GNN(depth=1, ResidualConnectionSimple(skip_steps=1))\n",
      "        GCNConvPyg[23 -> 17]\n",
      "        \n",
      "    \n",
      "        Task heads:\n",
      "        graph-task-1: NN-graph-task-1(depth=2, ResidualConnectionNone)\n",
      "            [FCLayer[17 -> 32 -> 3]\n",
      "        graph-task-2: NN-graph-task-2(depth=2, ResidualConnectionNone)\n",
      "            [FCLayer[17 -> 32 -> 4]\n",
      "        node-task-1: NN-node-task-1(depth=2, ResidualConnectionNone)\n",
      "            [FCLayer[17 -> 32 -> 2]\n",
      "\n",
      "\n",
      "graph-task-1 torch.Size([2, 3])\n",
      "graph-task-2 torch.Size([2, 4])\n",
      "node-task-1 torch.Size([7, 2])\n"
     ]
    }
   ],
   "source": [
    "graph = deepcopy(bg)\n",
    "print(graph.feat.shape)\n",
    "print(graph.edge_feat.shape)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(gnn_net)\n",
    "print(\"\\n\")\n",
    "\n",
    "out = gnn_net_edges(graph)\n",
    "for task in out.keys():\n",
    "    print(task, out[task].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4a99d018a205fcbcc0480c84566beaebcb91b08d0414b39a842df533e2a1d25"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('goli': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
