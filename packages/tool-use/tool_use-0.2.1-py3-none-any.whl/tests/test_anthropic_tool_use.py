import os
import unittest
from ast import literal_eval
from typing import Tuple

import pytest
import pytest_asyncio

from tool_use.tools.anthropic_tool_use import AnthropicConnection, anthropic_tool_use

# Run the unit tests
if __name__ == "__main__":
    from dotenv import load_dotenv

    load_dotenv()
    unittest.main()


@pytest_asyncio.fixture
def load_env():
    from dotenv import load_dotenv

    load_dotenv()


@pytest_asyncio.fixture
def my_connection(load_env) -> AnthropicConnection:
    return AnthropicConnection(
        secrets=dict(
            api_key=os.environ.get("ANTHROPIC_API_KEY"),
            api_type="anthropic",
            api_base="https://api.anthropic.com",
            client_key=None,
        ),
    )


@pytest_asyncio.fixture
def my_custom_connection(load_env) -> AnthropicConnection:
    return AnthropicConnection(
        secrets=dict(
            api_key=os.environ.get("ANTHROPIC_API_KEY"),
            api_type="anthropic",
            api_base=os.environ.get("ANTHROPIC_API_BASE"),
        ),
    )


def load_tool_prompts() -> Tuple[str, dict]:
    prompt = ""
    with open(
        os.path.join(os.path.dirname(__file__), "claude_tool_prompt.jinja2")
    ) as f:
        prompt = f.read()
    sample_tool = {}
    with open(os.path.join(os.path.dirname(__file__), "claude_tool.jinja2")) as f:
        sample_tool = f.read()
        sample_tool = literal_eval(sample_tool)
    return prompt, sample_tool


def load_prompt() -> str:
    prompt = ""
    with open(os.path.join(os.path.dirname(__file__), "claude_prompt.jinja2")) as f:
        prompt = f.read()
    return prompt


args = {
    "locale": "ko-KR",
    "current_datetime": "2024-04-17 16:17:11",
    "chat_history": [
        {"role": "user", "content": "ë¤¼íŠ¼ ì´ì„¸ì˜ì˜ ë³¸ê°€ëŠ”?"},
        {
            "role": "assistant",
            "content": "ì•Œê² ìŠµë‹ˆë‹¤. ë¤¼íŠ¼ ì´ì„¸ì˜ ëŒ€í‘œë‹˜ì˜ ë³¸ê°€ì— ëŒ€í•´ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n\nğŸ” ì •ë³´ ì¡°ì‚¬ ê²°ê³¼:\n- ì´ì„¸ì˜ ëŒ€í‘œë‹˜ì€ ì„œìš¸ì— ìœ„ì¹˜í•œ Wrtn Technologies, Inc.ì˜ ì°½ë¦½ìì´ì CEOì…ë‹ˆë‹¤. [[1]](https://kr.linkedin.com/in/seyoung-lee-noah/ko)\n- ê·¸ëŠ” ì—°ì„¸ëŒ€í•™êµë¥¼ ì¡¸ì—…í–ˆìœ¼ë©°, í˜„ì¬ 3,000ëª…ì˜ íŒ”ë¡œì›Œë¥¼ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. [[1]](https://kr.linkedin.com/in/seyoung-lee-noah/ko)\n- ì´ì„¸ì˜ ëŒ€í‘œë‹˜ì€ í•œêµ­í˜• ìƒì„± AI 'ë¤¼íŠ¼'ì˜ ëŒ€í‘œì´ê¸°ë„ í•©ë‹ˆë‹¤. ë¤¼íŠ¼ì€ ìµœê·¼ 100ë§Œ ëª…ì˜ ê°€ì…ìë¥¼ ëŒíŒŒí–ˆìŠµë‹ˆë‹¤. [[3]](https://www.idaegu.com/newsView/idg202309140064)\n- ì´ì„¸ì˜ ëŒ€í‘œë‹˜ì€ í¬ë¸ŒìŠ¤ì½”ë¦¬ì•„ê°€ ì„ ì •í•œ '30ì„¸ ë¯¸ë§Œ 30ì¸' ì¤‘ í•œ ëª…ìœ¼ë¡œ ì„ ì •ë˜ì—ˆìŠµë‹ˆë‹¤. [[4]](https://www.aitimes.com/news/articleView.html?idxno=157738)\n\nğŸ¤” ì¶”ê°€ ì •ë³´:\n- ì´ì„¸ì˜ ëŒ€í‘œë‹˜ì˜ ë³¸ê°€ ì •ë³´ëŠ” ê³µê°œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê·¸ê°€ ì„œìš¸ì— ìœ„ì¹˜í•œ Wrtn Technologies, Inc.ì˜ ì°½ë¦½ìë¼ëŠ” ì ì„ ê³ ë ¤í•  ë•Œ, ë³¸ê°€ ë˜í•œ ì„œìš¸ ê·¼ì²˜ì— ìˆì„ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.\n- ì´ì„¸ì˜ ëŒ€í‘œë‹˜ì€ ì—°ì„¸ëŒ€í•™êµ ì¶œì‹ ìœ¼ë¡œ, ë³¸ê°€ê°€ ì„œìš¸ ê·¼ì²˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì •í™•í•œ ìœ„ì¹˜ëŠ” ê³µê°œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\nğŸ™‚ ê²°ë¡ :\nì´ì„¸ì˜ ëŒ€í‘œë‹˜ì˜ ë³¸ê°€ ì •í™•í•œ ìœ„ì¹˜ëŠ” ê³µê°œë˜ì§€ ì•Šì•˜ì§€ë§Œ, ê·¸ê°€ ì„œìš¸ì— ìœ„ì¹˜í•œ Wrtn Technologies, Inc.ì˜ ì°½ë¦½ìì´ì ì—°ì„¸ëŒ€í•™êµ ì¶œì‹ ì´ë¼ëŠ” ì ì„ ê³ ë ¤í•  ë•Œ ì„œìš¸ ê·¼ì²˜ì— ìˆì„ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. ë” ìì„¸í•œ ì •ë³´ëŠ” ì¶”í›„ ê³µê°œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¶ê¸ˆí•œ ì ì´ ë” ìˆë‹¤ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”!",
        },
    ],
    "tool_calls": [
        {
            "id": "call_kRMGVaaeM5CJ2OuPbINXyUb7",
            "type": "function",
            "function": {
                "name": "browsing_with_google",
                "arguments": {
                    "search_query": "ë¤¼íŠ¼ ì´ì„¸ì˜ ë³¸ê°€ ìœ„ì¹˜",
                    "reason_for_selection": "To find the location of Luton Lee Se-young's hometown.",
                },
            },
        }
    ],
    "question": "ë¤¼íŠ¼ ì´ì„¸ì˜ì˜ ë³¸ê°€ëŠ”?",
    "tool_messages": [
        {
            "id": "call_kRMGVaaeM5CJ2OuPbINXyUb7",
            "function": {
                "name": "browsing_with_google",
                "arguments": {
                    "search_query": "ë¤¼íŠ¼ ì´ì„¸ì˜ ë³¸ê°€ ìœ„ì¹˜",
                    "reason_for_selection": "To find the location of Luton Lee Se-young's hometown.",
                },
            },
            "content": "[{'inner_text': '{\n  \"summary\": \"Yiseoung Lee is the founder and CEO of Wrtn Technologies, Inc., located in Seoul, South Korea. She is also an alumna of Yonsei University with 3,000 followers.\"\n}', 'res_idx': 1, 'link': 'https://kr.linkedin.com/in/seyoung-lee-noah/ko'}, {'inner_text': '{\n  \"title\": \"ì´ì„¸ì˜ ë¤¼íŠ¼ ëŒ€í‘œ ë“± AI ì „ë¬¸ê°€ 5ì¸, í¬ë¸ŒìŠ¤ì½”ë¦¬ì•„ %%'30ì„¸ ë¯¸ë§Œ 30ì¸%%' ì„ ì •\",\n  \"content\": \"í¬ë¸ŒìŠ¤ì½”ë¦¬ì•„ê°€ %%'30ì„¸ ë¯¸ë§Œ 30ì¸%%'ì— ì¸ê³µì§€ëŠ¥(AI) ì „ë¬¸ê°€ 5ëª…ì„ í¬í•¨ì‹œì¼°ë‹¤. ê¹€ì •í˜„, ê¹€í˜•ì¤€, ë°•ì°¬ì¤€, ì‹¬ê·œí˜„, ì´ì„¸ì˜ ë¤¼íŠ¼í…Œí¬ë†€ë¡œì§€ìŠ¤ ëŒ€í‘œê°€ ì£¼ìš” ì£¼ì¸ê³µìœ¼ë¡œ ì„ ì •ë˜ì—ˆë‹¤.\"\n}', 'res_idx': 2, 'link': 'https://www.aitimes.com/news/articleView.html?idxno=157738'}, {'inner_text': '{\n  \"summary\": \"The video is 21 minutes and 38 seconds long.\"\n}', 'res_idx': 3, 'link': 'https://www.youtube.com/watch?v=SYG-zG3o4fQ'}, {'inner_text': '{\n  \"store_guide\": \"Log in to the Chat Studio Store to access official character dialogues and set up your own character.\",\n  \"ai_search\": \"Lutens AI may generate inaccurate information due to referencing various data models, so it%%'s important to verify the accuracy of important information.\",\n  \"character_creation\": \"Create your own character with options like Hyesung Jeon, Taehoon Jang, Maeve, Cha Youngbin, Suni Chan, Minji Lee, and more.\",\n  \"prompt_options\": \"Explore prompt options like PowerPoint creation, initial email writing for marketing, game recommendations, Python interpreter, advertising copywriting, and persona generation.\"\n}', 'res_idx': 4, 'link': 'https://wrtn.ai/'}]",
        }
    ],
}


@pytest.mark.asyncio(scope="class")
class TestTool:
    async def test_anthropic_tool_use(self, my_connection):
        prompt, sample_tool = load_tool_prompts()
        result = await anthropic_tool_use(
            connection=my_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            tools=[sample_tool],
        )
        assert result is not None
        print(result)

    async def test_anthropic(self, my_connection):
        prompt = load_prompt()
        result = await anthropic_tool_use(
            connection=my_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            **args,
        )
        assert result is not None
        print(result)

    async def test_anthropic_stream(self, my_connection):
        prompt = load_prompt()
        result = await anthropic_tool_use(
            connection=my_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            stream=True,
            is_raw_output=True,
            **args,
        )
        chunk_count = 0
        total_result = ""
        async for chunk in result:
            chunk_count += 1
            if hasattr(chunk, "delta") and hasattr(chunk.delta, "text"):
                total_result += chunk.delta.text
        # raise Exception("This test is not working as expected")
        print(chunk_count)
        print(total_result)
        assert len(total_result) > 0

    async def test_anthropic_tool_use_2(self, my_custom_connection):
        prompt, sample_tool = load_tool_prompts()
        result = await anthropic_tool_use(
            connection=my_custom_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            tools=[sample_tool],
            stream=False,
        )
        assert result is not None
        print(result)

    async def test_anthropic_2(self, my_custom_connection):
        prompt = load_prompt()
        result = await anthropic_tool_use(
            connection=my_custom_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            stream=False,
            **args,
        )
        assert result is not None

    async def test_anthropic_stream_2(self, my_custom_connection):
        prompt = load_prompt()
        result = await anthropic_tool_use(
            connection=my_custom_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            stream=True,
            is_raw_output=False,
            extra_headers={"x-feature-id": "1"},
            **args,
        )
        chunk_count = 0
        total_result = ""
        total_result = ""
        async for chunk in result:
            chunk_count += 1
            if hasattr(chunk, "delta") and hasattr(chunk.delta, "text"):
                total_result += chunk.delta.text
        # raise Exception("This test is not working as expected")
        print(chunk_count)
        print(total_result)
        assert len(total_result) > 0, chunk_count


# Run the unit tests
if __name__ == "__main__":
    unittest.main()
