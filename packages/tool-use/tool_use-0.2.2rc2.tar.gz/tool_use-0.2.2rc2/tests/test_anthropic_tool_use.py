import os
import unittest
from ast import literal_eval
from typing import Tuple

import pytest
import pytest_asyncio

from tool_use.tools.anthropic_tool_use import (
    AnthropicConnection,
    _parse_chat,
    anthropic_tool_use,
    parse_template,
)

# Run the unit tests
if __name__ == "__main__":
    from dotenv import load_dotenv

    load_dotenv()
    unittest.main()


@pytest_asyncio.fixture
def load_env():
    from dotenv import load_dotenv

    load_dotenv()


@pytest_asyncio.fixture
def my_connection(load_env) -> AnthropicConnection:
    return AnthropicConnection(
        secrets=dict(
            api_key=os.environ.get("ANTHROPIC_API_KEY"),
            api_type="anthropic",
            api_base="https://api.anthropic.com",
            client_key=None,
        ),
    )


@pytest_asyncio.fixture
def my_custom_connection(load_env) -> AnthropicConnection:
    return AnthropicConnection(
        secrets=dict(
            api_key=os.environ.get("ANTHROPIC_API_KEY"),
            api_type="anthropic",
            api_base=os.environ.get("ANTHROPIC_API_BASE"),
        ),
    )


def load_tool_prompts() -> Tuple[str, dict]:
    prompt = ""
    with open(
        os.path.join(os.path.dirname(__file__), "claude_tool_prompt.jinja2")
    ) as f:
        prompt = f.read()
    sample_tool = {}
    with open(os.path.join(os.path.dirname(__file__), "claude_tool.jinja2")) as f:
        sample_tool = f.read()
        sample_tool = literal_eval(sample_tool)
    return prompt, sample_tool


def load_prompt() -> str:
    prompt = ""
    with open(os.path.join(os.path.dirname(__file__), "claude_prompt.jinja2")) as f:
        prompt = f.read()
    return prompt


args = {
    "locale": "ko-KR",
    "current_datetime": "2024-04-17 16:17:11",
    "chat_history": [
        {"role": "user", "content": "ë¤¼íŠ¼ ì´ì„¸ì˜ì˜ ë³¸ê°€ëŠ”?"},
        {
            "role": "assistant",
            "content": "ì•Œê² ìŠµë‹ˆë‹¤. ë¤¼íŠ¼ ì´ì„¸ì˜ ëŒ€í‘œë‹˜ì˜ ë³¸ê°€ì— ëŒ€í•´ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n\nğŸ” ì •ë³´ ì¡°ì‚¬ ê²°ê³¼:\n- ì´ì„¸ì˜ ëŒ€í‘œë‹˜ì€ ì„œìš¸ì— ìœ„ì¹˜í•œ Wrtn Technologies, Inc.ì˜ ì°½ë¦½ìì´ì CEOì…ë‹ˆë‹¤. [[1]](https://kr.linkedin.com/in/seyoung-lee-noah/ko)\n- ê·¸ëŠ” ì—°ì„¸ëŒ€í•™êµë¥¼ ì¡¸ì—…í–ˆìœ¼ë©°, í˜„ì¬ 3,000ëª…ì˜ íŒ”ë¡œì›Œë¥¼ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. [[1]](https://kr.linkedin.com/in/seyoung-lee-noah/ko)\n- ì´ì„¸ì˜ ëŒ€í‘œë‹˜ì€ í•œêµ­í˜• ìƒì„± AI 'ë¤¼íŠ¼'ì˜ ëŒ€í‘œì´ê¸°ë„ í•©ë‹ˆë‹¤. ë¤¼íŠ¼ì€ ìµœê·¼ 100ë§Œ ëª…ì˜ ê°€ì…ìë¥¼ ëŒíŒŒí–ˆìŠµë‹ˆë‹¤. [[3]](https://www.idaegu.com/newsView/idg202309140064)\n- ì´ì„¸ì˜ ëŒ€í‘œë‹˜ì€ í¬ë¸ŒìŠ¤ì½”ë¦¬ì•„ê°€ ì„ ì •í•œ '30ì„¸ ë¯¸ë§Œ 30ì¸' ì¤‘ í•œ ëª…ìœ¼ë¡œ ì„ ì •ë˜ì—ˆìŠµë‹ˆë‹¤. [[4]](https://www.aitimes.com/news/articleView.html?idxno=157738)\n\nğŸ¤” ì¶”ê°€ ì •ë³´:\n- ì´ì„¸ì˜ ëŒ€í‘œë‹˜ì˜ ë³¸ê°€ ì •ë³´ëŠ” ê³µê°œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê·¸ê°€ ì„œìš¸ì— ìœ„ì¹˜í•œ Wrtn Technologies, Inc.ì˜ ì°½ë¦½ìë¼ëŠ” ì ì„ ê³ ë ¤í•  ë•Œ, ë³¸ê°€ ë˜í•œ ì„œìš¸ ê·¼ì²˜ì— ìˆì„ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.\n- ì´ì„¸ì˜ ëŒ€í‘œë‹˜ì€ ì—°ì„¸ëŒ€í•™êµ ì¶œì‹ ìœ¼ë¡œ, ë³¸ê°€ê°€ ì„œìš¸ ê·¼ì²˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì •í™•í•œ ìœ„ì¹˜ëŠ” ê³µê°œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\nğŸ™‚ ê²°ë¡ :\nì´ì„¸ì˜ ëŒ€í‘œë‹˜ì˜ ë³¸ê°€ ì •í™•í•œ ìœ„ì¹˜ëŠ” ê³µê°œë˜ì§€ ì•Šì•˜ì§€ë§Œ, ê·¸ê°€ ì„œìš¸ì— ìœ„ì¹˜í•œ Wrtn Technologies, Inc.ì˜ ì°½ë¦½ìì´ì ì—°ì„¸ëŒ€í•™êµ ì¶œì‹ ì´ë¼ëŠ” ì ì„ ê³ ë ¤í•  ë•Œ ì„œìš¸ ê·¼ì²˜ì— ìˆì„ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. ë” ìì„¸í•œ ì •ë³´ëŠ” ì¶”í›„ ê³µê°œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¶ê¸ˆí•œ ì ì´ ë” ìˆë‹¤ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”!",
        },
    ],
    "tool_calls": [
        {
            "id": "call_kRMGVaaeM5CJ2OuPbINXyUb7",
            "type": "function",
            "function": {
                "name": "browsing_with_google",
                "arguments": {
                    "search_query": "ë¤¼íŠ¼ ì´ì„¸ì˜ ë³¸ê°€ ìœ„ì¹˜",
                    "reason_for_selection": "To find the location of Luton Lee Se-young's hometown.",
                },
            },
        }
    ],
    "question": "ë¤¼íŠ¼ ì´ì„¸ì˜ì˜ ë³¸ê°€ëŠ”?",
    "tool_messages": [
        {
            "id": "call_kRMGVaaeM5CJ2OuPbINXyUb7",
            "function": {
                "name": "browsing_with_google",
                "arguments": {
                    "search_query": "ë¤¼íŠ¼ ì´ì„¸ì˜ ë³¸ê°€ ìœ„ì¹˜",
                    "reason_for_selection": "To find the location of Luton Lee Se-young's hometown.",
                },
            },
            "content": "[{'inner_text': '{\n  \"summary\": \"Yiseoung Lee is the founder and CEO of Wrtn Technologies, Inc., located in Seoul, South Korea. She is also an alumna of Yonsei University with 3,000 followers.\"\n}', 'res_idx': 1, 'link': 'https://kr.linkedin.com/in/seyoung-lee-noah/ko'}, {'inner_text': '{\n  \"title\": \"ì´ì„¸ì˜ ë¤¼íŠ¼ ëŒ€í‘œ ë“± AI ì „ë¬¸ê°€ 5ì¸, í¬ë¸ŒìŠ¤ì½”ë¦¬ì•„ %%'30ì„¸ ë¯¸ë§Œ 30ì¸%%' ì„ ì •\",\n  \"content\": \"í¬ë¸ŒìŠ¤ì½”ë¦¬ì•„ê°€ %%'30ì„¸ ë¯¸ë§Œ 30ì¸%%'ì— ì¸ê³µì§€ëŠ¥(AI) ì „ë¬¸ê°€ 5ëª…ì„ í¬í•¨ì‹œì¼°ë‹¤. ê¹€ì •í˜„, ê¹€í˜•ì¤€, ë°•ì°¬ì¤€, ì‹¬ê·œí˜„, ì´ì„¸ì˜ ë¤¼íŠ¼í…Œí¬ë†€ë¡œì§€ìŠ¤ ëŒ€í‘œê°€ ì£¼ìš” ì£¼ì¸ê³µìœ¼ë¡œ ì„ ì •ë˜ì—ˆë‹¤.\"\n}', 'res_idx': 2, 'link': 'https://www.aitimes.com/news/articleView.html?idxno=157738'}, {'inner_text': '{\n  \"summary\": \"The video is 21 minutes and 38 seconds long.\"\n}', 'res_idx': 3, 'link': 'https://www.youtube.com/watch?v=SYG-zG3o4fQ'}, {'inner_text': '{\n  \"store_guide\": \"Log in to the Chat Studio Store to access official character dialogues and set up your own character.\",\n  \"ai_search\": \"Lutens AI may generate inaccurate information due to referencing various data models, so it%%'s important to verify the accuracy of important information.\",\n  \"character_creation\": \"Create your own character with options like Hyesung Jeon, Taehoon Jang, Maeve, Cha Youngbin, Suni Chan, Minji Lee, and more.\",\n  \"prompt_options\": \"Explore prompt options like PowerPoint creation, initial email writing for marketing, game recommendations, Python interpreter, advertising copywriting, and persona generation.\"\n}', 'res_idx': 4, 'link': 'https://wrtn.ai/'}]",
        }
    ],
}


@pytest.mark.asyncio(scope="class")
class TestTool:
    async def test_anthropic(self, my_connection):
        prompt = load_prompt()
        result = await anthropic_tool_use(
            connection=my_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            **args,
        )
        assert result is not None
        print(result)

    async def test_anthropic_stream(self, my_connection):
        prompt = load_prompt()
        result = await anthropic_tool_use(
            connection=my_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            stream=True,
            is_raw_output=True,
            **args,
        )
        chunk_count = 0
        total_result = ""
        async for chunk in result:
            chunk_count += 1
            if hasattr(chunk, "delta") and hasattr(chunk.delta, "text"):
                total_result += chunk.delta.text
        # raise Exception("This test is not working as expected")
        print(chunk_count)
        print(total_result)
        assert len(total_result) > 0

    async def test_anthropic_tool_use(self, my_connection):
        prompt, sample_tool = load_tool_prompts()
        result = await anthropic_tool_use(
            connection=my_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            tools=[sample_tool],
        )
        assert result is not None
        print(result)

    async def test_anthropic_tool_use_2(self, my_connection):
        prompt, sample_tool = load_tool_prompts()
        result = await anthropic_tool_use(
            connection=my_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            tools=[sample_tool],
            stream=False,
        )
        assert result is not None
        print(result)

    async def test_anthropic_tool_use_3(self, my_connection):
        prompt, sample_tool = load_tool_prompts()
        result = await anthropic_tool_use(
            connection=my_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            tools=[sample_tool],
            tool_choice={"type": "tool", "name": "search_recipes"},
            stream=False,
        )
        assert result is not None
        print(result)

    async def test_anthropic_2(self, my_custom_connection):
        prompt = load_prompt()
        result = await anthropic_tool_use(
            connection=my_custom_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            stream=False,
            **args,
        )
        assert result is not None

    async def test_anthropic_stream_2(self, my_custom_connection):
        prompt = load_prompt()
        result = await anthropic_tool_use(
            connection=my_custom_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            stream=True,
            is_raw_output=False,
            extra_headers={"x-feature-id": "1"},
            **args,
        )
        chunk_count = 0
        total_result = ""
        total_result = ""
        async for chunk in result:
            chunk_count += 1
            if hasattr(chunk, "delta") and hasattr(chunk.delta, "text"):
                total_result += chunk.delta.text
        # raise Exception("This test is not working as expected")
        print(chunk_count)
        print(total_result)
        assert len(total_result) > 0, chunk_count

    async def test_anthropic_tool_use_b(self, my_custom_connection):
        prompt, sample_tool = load_tool_prompts()
        result = await anthropic_tool_use(
            connection=my_custom_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            tools=[sample_tool],
        )
        assert result is not None
        print(result)

    async def test_anthropic_tool_use_b_2(self, my_custom_connection):
        prompt, sample_tool = load_tool_prompts()
        result = await anthropic_tool_use(
            connection=my_custom_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            tools=[sample_tool],
            stream=False,
        )
        assert result is not None
        print(result)

    async def test_anthropic_tool_use_b_3(self, my_custom_connection):
        prompt, sample_tool = load_tool_prompts()
        result = await anthropic_tool_use(
            connection=my_custom_connection,
            prompt=prompt,
            model="claude-3-haiku-20240307",
            max_tokens=2000,
            tools=[sample_tool],
            tool_choice={"type": "tool", "name": "search_recipes"},
            stream=False,
        )
        assert result is not None
        print(result)

    async def test_colon_starting_text(self, my_connection):
        prompt = """
system:
you are an AI asistant that helps to people to give the information.

user:
type:text
text:{question}

type:image
source:{{"type": "base64", "media_type": "image/jpeg", "data": "asdfkwwiwi"}}
""".strip().format(
            question="ì´ ê·¸ë¦¼ì´ ë­”ì§€ ì„¤ëª…í•´ì¤˜\nì˜ ì„¤ëª…í•´ì¤˜"
        )

        prompt2 = """
system:
you are an AI asistant that helps to people to give the information.

user:
type:ì•ˆë…•í•˜ì„¸ìš”.ì €ë… ë©”ë‰´ ì¶”ì²œí•´ì£¼ì„¸ìš”.\në­ ë¨¹ì„ê¹Œìš”?""".strip()

        valid_roles = ["system", "user", "assistant"]
        messages, is_tool_use = parse_template(prompt2, valid_roles)
        system, chat_list, is_tool_use = _parse_chat(prompt)

        # single user message with content type text and image
        assert chat_list == [
            {
                "role": "user",
                "content": [
                    {"text": "ì´ ê·¸ë¦¼ì´ ë­”ì§€ ì„¤ëª…í•´ì¤˜", "type": "text"},
                    {
                        "source": {
                            "type": "base64",
                            "media_type": "image/jpeg",
                            "data": "asdfkwwiwi",
                        },
                        "type": "image",
                    },
                ],
            }
        ]
        assert messages == [
            {
                "role": "system",
                "content": [
                    {
                        "type": "text",
                        "text": "you are an AI asistant that helps to people to give the information.\n",
                    }
                ],
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "type:ì•ˆë…•í•˜ì„¸ìš”.ì €ë… ë©”ë‰´ ì¶”ì²œí•´ì£¼ì„¸ìš”.\në­ ë¨¹ì„ê¹Œìš”?"}
                ],
            },
        ]


# Run the unit tests
if __name__ == "__main__":
    unittest.main()
