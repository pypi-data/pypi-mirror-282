# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/06_callbacks.ipynb.

# %% auto 0
__all__ = ['def_device', 'to_device', 'to_cpu', 'Callback', 'run_cbs', 'BaseSchedCB', 'BatchSchedCB', 'BatchTransformCB',
           'DeviceCB', 'MetricsCB', 'ProgressCB', 'SingleBatchCB', 'TrainCB', 'CancelFitException',
           'CancelBatchException', 'CancelEpochException', 'LRFinderCB']

# %% ../nbs/06_callbacks.ipynb 3
from copy import copy
import math
import fastcore.all as fc
from fastprogress import progress_bar, master_bar
from operator import attrgetter
from collections.abc import Mapping

import matplotlib.pyplot as plt

import torch
from torchvision import transforms as T
from torcheval.metrics import Mean

# %% ../nbs/06_callbacks.ipynb 4
def_device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'

def to_device(x, device=def_device):
    if isinstance(x, torch.Tensor): return x.to(device)
    if isinstance(x, Mapping): return {k:v.to(device) for k,v in x.items()}
    return type(x)(to_device(o, device) for o in x)

# %% ../nbs/06_callbacks.ipynb 5
def to_cpu(x):
    if isinstance(x, Mapping): return {k:to_cpu(v) for k,v in x.items()}
    if isinstance(x, list): return [to_cpu(o) for o in x]
    if isinstance(x, tuple): return tuple(to_cpu(list(x)))
    res = x.detach().cpu()
    return res.float() if res.dtype==torch.float16 else res

# %% ../nbs/06_callbacks.ipynb 6
class Callback(): order = 0

# %% ../nbs/06_callbacks.ipynb 7
def run_cbs(
    cbs:list, # Callbacks to run
    method_nm:str, # Name of the method to call on all passed callbacks
    learn=None # Contect object in which the callbacks might be used
    ):
    "Run callbacks in the order defined in their `order` class attribute."
    for cb in sorted(cbs, key=attrgetter('order')):
        method = getattr(cb, method_nm, None)
        if method is not None: method(learn)

# %% ../nbs/06_callbacks.ipynb 12
class BaseSchedCB(Callback):
    def __init__(self, sched): self.sched = sched
    def before_fit(self, learn): self.schedo = self.sched(learn.opt)
    def _step(self, learn):
        if learn.training: self.schedo.step()

# %% ../nbs/06_callbacks.ipynb 13
class BatchSchedCB(BaseSchedCB):
    def after_batch(self, learn): self._step(learn)

# %% ../nbs/06_callbacks.ipynb 14
class BatchTransformCB(Callback):
    def __init__(self, tfm, on_train=True, on_val=True): fc.store_attr()

    def before_batch(self, learn):
        if (self.on_train and learn.training) or (self.on_val and not learn.training):
            learn.batch = self.tfm(learn.batch)

# %% ../nbs/06_callbacks.ipynb 15
class DeviceCB(Callback):
    "Put `model` (before_fit) or `batch` (before_batch) to `device`."
    def __init__(self, 
                 device:str=def_device, # 'cpu, 'mps', 'cuda', ...
                 ): 
        fc.store_attr()
        
    def before_fit(self, learn): 
        if hasattr(learn.model, 'to'): 
            learn.model.to(self.device)
    def before_batch(self, learn): 
        learn.batch = to_device(learn.batch, device=self.device)

# %% ../nbs/06_callbacks.ipynb 16
class MetricsCB(Callback):
    "Metrics callback."
    def __init__(self, *ms, **metrics):
        for o in ms: metrics[type(o).__name__] = o
        self.metrics = metrics
        self.all_metrics = copy(metrics)
        self.all_metrics['loss'] = self.loss = Mean()

    def _log(self, d): print(d)
    def before_fit(self, learn): learn.metrics = self
    def before_epoch(self, learn): [o.reset() for o in self.all_metrics.values()]

    def after_epoch(self, learn):
        log = {k:f'{v.compute():.3f}' for k,v in self.all_metrics.items()}
        # log['epoch'] = learn.epoch
        log['epoch'] = str(learn.epoch)
        log['train'] = 'train' if learn.model.training else 'eval'
        self._log(log)

    def after_batch(self, learn):
        x,y,*_ = to_cpu(learn.batch)
        for m in self.metrics.values(): m.update(to_cpu(learn.preds), y)
        self.loss.update(to_cpu(learn.loss), weight=len(x))

# %% ../nbs/06_callbacks.ipynb 17
class ProgressCB(Callback):
    order = MetricsCB.order+1
    def __init__(self, plot=False): self.plot = plot
    def before_fit(self, learn):
        learn.epochs = self.mbar = master_bar(learn.epochs)
        self.first = True
        if hasattr(learn, 'metrics'): learn.metrics._log = self._log
        self.losses = []
        self.val_losses = []

    def _log(self, d):
        if self.first:
            self.mbar.write(list(d), table=True)
            self.first = False
        self.mbar.write(list(d.values()), table=True)

    def before_epoch(self, learn): 
        learn.dl = progress_bar(learn.dl, leave=False, parent=self.mbar)
        
    def after_batch(self, learn):
        learn.dl.comment = f'{learn.loss:.3f}'
        if self.plot and hasattr(learn, 'metrics') and learn.training:
            self.losses.append(learn.loss.item())
            if self.val_losses: 
                self.mbar.update_graph([[
                    fc.L.range(self.losses), self.losses],
                                        [fc.L.range(learn.epoch).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])
    
    def after_epoch(self, learn): 
        if not learn.training:
            if self.plot and hasattr(learn, 'metrics'): 
                self.val_losses.append(learn.metrics.all_metrics['loss'].compute())
                self.mbar.update_graph([[fc.L.range(self.losses), self.losses],
                                        [fc.L.range(learn.epoch+1).map(lambda x: (x+1)*len(learn.dls.train)), self.val_losses]])

# %% ../nbs/06_callbacks.ipynb 18
class SingleBatchCB(Callback):
    order = 1
    def after_batch(self, learn): raise CancelFitException()

# %% ../nbs/06_callbacks.ipynb 19
class TrainCB(Callback):
    def __init__(self, n_inp=1): self.n_inp = n_inp
    def predict(self, learn): 
        learn.preds = learn.model(*learn.batch[:self.n_inp])
        
    def get_loss(self, learn): 
        learn.loss = learn.loss_func(learn.preds, *learn.batch[self.n_inp:self.n_inp+1])
        
    def backward(self, learn): learn.loss.backward()
    def step(self, learn): learn.opt.step()
    def zero_grad(self, learn): learn.opt.zero_grad()

# %% ../nbs/06_callbacks.ipynb 20
from torch.optim.lr_scheduler import ExponentialLR

# %% ../nbs/06_callbacks.ipynb 21
class CancelFitException(Exception): pass
class CancelBatchException(Exception): pass 
class CancelEpochException(Exception): pass

# %% ../nbs/06_callbacks.ipynb 22
class LRFinderCB(Callback):
    def __init__(self, gamma=1.3, max_mult=3): fc.store_attr()
    
    def before_fit(self, learn):
        self.sched = ExponentialLR(learn.opt, self.gamma)
        self.lrs,self.losses = [],[]
        self.min = math.inf

    def after_batch(self, learn):
        if not learn.training: raise CancelEpochException()
        self.lrs.append(learn.opt.param_groups[0]['lr'])
        loss = to_cpu(learn.loss)
        self.losses.append(loss)
        if loss < self.min: self.min = loss
        if math.isnan(loss) or (loss > self.min*self.max_mult):
            raise CancelFitException()
        self.sched.step()

    def cleanup_fit(self, learn):
        plt.plot(self.lrs, self.losses)
        plt.xscale('log')
