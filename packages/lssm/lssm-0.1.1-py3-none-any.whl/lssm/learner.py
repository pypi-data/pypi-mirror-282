# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/05_learner.ipynb.

# %% auto 0
__all__ = ['with_cbs', 'Learner', 'lr_find']

# %% ../nbs/05_learner.ipynb 4
from functools import partial

import fastcore.all as fc
from collections.abc import Mapping

import torch
from torch import optim, nn
import torch.nn.functional as F
from torchvision import transforms as T
from .callbacks import (run_cbs, to_cpu, LRFinderCB,
                            CancelBatchException, CancelFitException, CancelEpochException)

# %% ../nbs/05_learner.ipynb 6
class with_cbs:
    'Decorator calling "before_`nm`" and "after_`nm`" around the decorated method. Call "cleanup_`nm`" once done.'
    def __init__(self, 
                 nm:str # Name of the callback method to call
                 ): 
        self.nm = nm
    def __call__(self, f):
        def _f(o, *args, **kwargs):
            try:
                o.callback(f'before_{self.nm}')
                f(o, *args, **kwargs)
                o.callback(f'after_{self.nm}')
            except globals()[f'Cancel{self.nm.title()}Exception']: pass
            finally: o.callback(f'cleanup_{self.nm}')
        return _f

# %% ../nbs/05_learner.ipynb 7
class Learner():
    def __init__(self,
                 model, 
                 dls=(0,),
                 loss_func=F.mse_loss,
                 lr=0.1,
                 cbs=None,
                 opt_func=optim.SGD
                 ):
        cbs = fc.L(cbs)
        fc.store_attr()

    @with_cbs('batch')
    def _one_batch(self):
        self.predict()
        self.callback('after_predict')
        self.get_loss()
        self.callback('after_loss')
        if self.training:
            self.backward()
            self.callback('after_backward')
            self.step()
            self.callback('after_step')
            self.zero_grad()

    @with_cbs('epoch')
    def _one_epoch(self):
        for self.iter,self.batch in enumerate(self.dl): 
            self._one_batch()

    def one_epoch(self, training):
        self.model.train(training)
        self.dl = self.dls.train if training else self.dls.valid
        self._one_epoch()

    @with_cbs('fit')
    def _fit(self, train, valid):
        for self.epoch in self.epochs:
            if train: self.one_epoch(True)
            if valid: torch.no_grad()(self.one_epoch)(False)

    def fit(self, n_epochs=1, train=True, valid=True, cbs=None, lr=None):
        cbs = fc.L(cbs)
        # `add_cb` and `rm_cb` were added in lesson 18
        for cb in cbs: self.cbs.append(cb)
        try:
            self.n_epochs = n_epochs
            self.epochs = range(n_epochs)
            if lr is None: lr = self.lr
            if self.opt_func: self.opt = self.opt_func(self.model.parameters(), lr)
            self._fit(train, valid)
        finally:
            for cb in cbs: self.cbs.remove(cb)

    def _batch_preds(self, X):
        X = torch.from_numpy(X).to(dtype=torch.float32).view(X.shape[0], 1, X.shape[-1])
        y_placeholder = torch.empty((len(X), 1), dtype=torch.float32)
        return [X, y_placeholder]

    def get_preds(self, X, y_tfm_fn=None):
        self.model.train(False)
        self.batch = self._batch_preds(X)
        self._one_batch()
        y_hat = self.preds.detach().numpy()
        return y_tfm_fn(y_hat) if y_tfm_fn else y_hat
        
    def __getattr__(self, name):
        if name in ('predict','get_loss','backward','step','zero_grad'): return partial(self.callback, name)
        raise AttributeError(name)

    def callback(self, method_nm): run_cbs(self.cbs, method_nm, self)
    
    @property
    def training(self): return self.model.training

# %% ../nbs/05_learner.ipynb 11
@fc.patch
def lr_find(self:Learner, gamma=1.3, max_mult=3, start_lr=1e-5, max_epochs=10):
    self.fit(max_epochs, lr=start_lr, cbs=LRFinderCB(gamma=gamma, max_mult=max_mult))
